{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the OD Matrices\n",
    "- Baseline: OD matrix with limitation factor lambda (Anastassia)\n",
    "- Scenario 1: OD matrix  with pop density, deterrence factor and limitation factor lambda (Baseline Yap et al. + Anastassia)\n",
    "- Scenario 2/3/4: OD matrix with vulnerability  (median income, education level, # jobs within 30min with competition), pop density and deterrence factor lambda (modified Yap et al. + Anastassia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import osmnx as nx\n",
    "import shapely\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "import math\n",
    "import igraph as ig\n",
    "\n",
    "crs_fr = 2154\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (Anastassia)\n",
    "%run -i packages.py\n",
    "def make_attr_dict(*args, **kwargs): \n",
    "    \n",
    "    argCount = len(kwargs)\n",
    "    \n",
    "    if argCount > 0:\n",
    "        attributes = {}\n",
    "        for kwarg in kwargs:\n",
    "            attributes[kwarg] = kwargs.get(kwarg, None)\n",
    "        return attributes\n",
    "    else:\n",
    "        return None # (if no attributes are given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Shapes\n",
    "\n",
    "# GPM outline\n",
    "GPM = gpd.read_file('data/raw/GPM.geojson').to_crs(crs_fr)\n",
    "\n",
    "# IRIS codes and shapes \n",
    "IRIS_GPM = gpd.read_file('data/raw/IRIS_GPM.geojson')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network and adding igraph IDs to the node table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-20\n"
     ]
    }
   ],
   "source": [
    "#--- Create the network\n",
    "# Retrieve edges\n",
    "edges_with_id = pd.read_csv('data/clean/initial_network_edges_complete.csv')\n",
    "edges_with_id[\"geometry\"] = edges_with_id.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "edges_with_id = gpd.GeoDataFrame(edges_with_id, geometry = 'geometry', crs = 4326).to_crs(2154)\n",
    "\n",
    "# Retrieve nodes\n",
    "nodes_carbike_centroids_RER_complete = pd.read_csv('data/clean/initial_network_nodes_school_complete.csv')\n",
    "nodes_carbike_centroids_RER_complete[\"geometry\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "nodes_carbike_centroids_RER_complete = gpd.GeoDataFrame(nodes_carbike_centroids_RER_complete, geometry = 'geometry', crs = 2154)\n",
    "\n",
    "# Create the attr_dict\n",
    "nodes_carbike_centroids_RER_complete[\"attr_dict\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: make_attr_dict(\n",
    "                                                                  nodetype = x.nodetype,\n",
    "                                                                  centroid = x.centroid,\n",
    "                                                                  RER = x.RER,\n",
    "                                                                  IRIS = x.CODE_IRIS,\n",
    "                                                                  pop_dens = x.pop_density),\n",
    "                                                                  axis = 1) \n",
    "\n",
    "#--- Create Graph with all nodes and edges\n",
    "G = nx.from_pandas_edgelist(edges_with_id, source='x', target='y', edge_attr=True)\n",
    "G.add_nodes_from(nodes_carbike_centroids_RER_complete.loc[:,[\"osmid\", \"attr_dict\"]].itertuples(index = False))\n",
    "\n",
    "#--- Check if all nodes and edges are present\n",
    "# Both should be 0\n",
    "print(len(G.nodes()) - len(nodes_carbike_centroids_RER_complete))\n",
    "print(len(G.edges()) - len(edges_with_id))\n",
    "\n",
    "#--- Moving from NetworkX to igraph\n",
    "g_igraph = ig.Graph()\n",
    "networkx_graph = G\n",
    "g_igraph = ig.Graph.from_networkx(networkx_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the basic OD matrix: O, D, lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/lkz691q11b72bpprh6_v34_h0000gn/T/ipykernel_1781/982786006.py:19: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "  shortest_path_length = g_igraph.shortest_paths_dijkstra(source=start_node, target=end_node, weights='weight')[0][0]\n",
      "/var/folders/2m/lkz691q11b72bpprh6_v34_h0000gn/T/ipykernel_1781/982786006.py:19: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "  shortest_path_length = g_igraph.shortest_paths_dijkstra(source=start_node, target=end_node, weights='weight')[0][0]\n",
      "/var/folders/2m/lkz691q11b72bpprh6_v34_h0000gn/T/ipykernel_1781/982786006.py:19: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "  shortest_path_length = g_igraph.shortest_paths_dijkstra(source=start_node, target=end_node, weights='weight')[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         49.74117985 50.25135853]\n",
      " [49.74117985  0.         56.34103806]\n",
      " [50.25135853 56.34103806  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#TODO add lambda thing! \n",
    "#TODO add transport mode limitation. you can't walk/bike/walk/bike in alternation \n",
    "\n",
    "# Filter nodes for centroids\n",
    "seq = g_igraph.vs.select(centroid_eq = True)\n",
    "centroids = [v.index for v in seq]\n",
    "# Subset if necessary (testing purposes)\n",
    "nodes = centroids[0:3]\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    unique_lst = []\n",
    "    for sublist in lst:\n",
    "        if sorted(sublist) not in [sorted(unique_sublist) for unique_sublist in unique_lst]:\n",
    "            unique_lst.append(sublist)\n",
    "    return unique_lst\n",
    "\n",
    "def process_node(args):\n",
    "    start_node, end_node = args\n",
    "    shortest_path_length = g_igraph.shortest_paths_dijkstra(source=start_node, target=end_node, weights='weight')[0][0]\n",
    "    return (start_node, end_node, shortest_path_length)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Number of processes (cores) to use for parallel processing\n",
    "    num_processes = 4\n",
    "\n",
    "    # Create a pool of processes\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Generate combinations of nodes for processing\n",
    "    node_combinations = [(start_node, end_node) for start_node in nodes for end_node in nodes if start_node != end_node]\n",
    "    node_combinations = remove_duplicates(node_combinations)\n",
    "\n",
    "    # Apply the function to each node combination using parallel processing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Create a dictionary to store the shortest path lengths\n",
    "    output = {}\n",
    "    for start_node, end_node, shortest_path_length in results:\n",
    "        if start_node not in output:\n",
    "            output[start_node] = {}\n",
    "        output[start_node][end_node] = shortest_path_length\n",
    "\n",
    "    # Create an empty adjacency matrix\n",
    "    matrix = np.zeros((len(nodes), len(nodes)))\n",
    "\n",
    "    # Fill the adjacency matrix with shortest path lengths\n",
    "    for i, start_node in enumerate(nodes):\n",
    "        for j, end_node in enumerate(nodes):\n",
    "            if start_node in output and end_node in output[start_node]:\n",
    "                matrix[i, j] = output[start_node][end_node]\n",
    "                matrix[j, i] = output[start_node][end_node]\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the basic OD matrix from Yap et al.: O, D, lambda, population density, deterrence factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: do I need lambda if I have a detterence factor?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Extract the population density from all nodes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m nodes:\n\u001b[0;32m----> 8\u001b[0m     pop_density \u001b[39m=\u001b[39m nodes_carbike_centroids_RER_complete\u001b[39m.\u001b[39;49mloc[nodes_carbike_centroids_RER_complete[\u001b[39m'\u001b[39;49m\u001b[39mosmid\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m node][\u001b[39m'\u001b[39;49m\u001b[39mpop_density\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49miloc[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m      9\u001b[0m     node_pop_density[node] \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mpop_density\u001b[39m\u001b[39m'\u001b[39m: pop_density}\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1070\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m   1072\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m-> 1073\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/indexing.py:1625\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index by location index with a non-integer key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1624\u001b[0m \u001b[39m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1625\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_integer(key, axis)\n\u001b[1;32m   1627\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_ixs(key, axis\u001b[39m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/site-packages/pandas/core/indexing.py:1557\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m len_axis \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m len_axis \u001b[39mor\u001b[39;00m key \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1557\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msingle positional indexer is out-of-bounds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "#--- Retrieve population density\n",
    "# TODO adapt to igraph IDs instead of OSMids\n",
    "# Create a dictionary to store the node attributes\n",
    "node_pop_density = {}\n",
    "\n",
    "# Extract the population density from all nodes\n",
    "for node in nodes:\n",
    "    pop_density = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['osmid'] == node]['pop_density'].iloc[0]\n",
    "    node_pop_density[node] = {'pop_density': pop_density}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "38956",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m start_node \u001b[39m=\u001b[39m nodes[i]\n\u001b[1;32m     13\u001b[0m end_node \u001b[39m=\u001b[39m nodes[j]\n\u001b[0;32m---> 14\u001b[0m A \u001b[39m=\u001b[39m node_pop_density[start_node][\u001b[39m'\u001b[39m\u001b[39mpop_density\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m B \u001b[39m=\u001b[39m node_pop_density[end_node][\u001b[39m'\u001b[39m\u001b[39mpop_density\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     16\u001b[0m OD_matrix_original[i, j] \u001b[39m=\u001b[39m A \u001b[39m*\u001b[39m B \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mmatrix[i, j] \u001b[39m/\u001b[39m max_value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 38956"
     ]
    }
   ],
   "source": [
    "#--- Calculate OD matrix \n",
    "#TODO adapt to igraph IDs\n",
    "\n",
    "# Create an empty matrix\n",
    "OD_matrix_original = np.zeros_like(matrix) \n",
    "\n",
    "# Find the maximum value of the adjacency matrix (needed for the deterrence factor)\n",
    "max_value = np.max(matrix)\n",
    "\n",
    "# Fill in the matrix: (population density i) * (population density j) * exp(- shortest path length / maximum possible shortest path length)\n",
    "for i in range(len(nodes)):\n",
    "    for j in range(len(nodes)):\n",
    "        start_node = nodes[i]\n",
    "        end_node = nodes[j]\n",
    "        A = node_pop_density[start_node]['pop_density']\n",
    "        B = node_pop_density[end_node]['pop_density']\n",
    "        OD_matrix_original[i, j] = A * B * np.exp(-matrix[i, j] / max_value)\n",
    "\n",
    "print(OD_matrix_original)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create modified OD matrix with economic vulnerability: use median income as the vulnerability attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
