{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the OD Matrices\n",
    "- Matrix 0: shortest trips between centroids\n",
    "- Baseline: pop density and exp(normalized distance) -> gravity model baseline like Yap et al.\n",
    "- Matrix set 1: equalizing for median income, education level, number of schools and number of jobs SEPARATELY\n",
    "- Matrix set 2: equalizing for different attributes in O and D. O/D equalized for education level/number of schools, median income/number of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import networkx as nx\n",
    "import shapely\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "import igraph as ig\n",
    "from ta_lab.assignment.assign import frank_wolfe\n",
    "from ta_lab.assignment.line import *\n",
    "from ta_lab.assignment.graph import *\n",
    "from ta_lab.assignment.shortest_path import ShortestPath as SPP\n",
    "\n",
    "crs_fr = 2154\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (Anastassia)\n",
    "# Create a dictionary of attributes (useful for networkX)\n",
    "%run -i packages.py\n",
    "def make_attr_dict(*args, **kwargs): \n",
    "    \n",
    "    argCount = len(kwargs)\n",
    "    \n",
    "    if argCount > 0:\n",
    "        attributes = {}\n",
    "        for kwarg in kwargs:\n",
    "            attributes[kwarg] = kwargs.get(kwarg, None)\n",
    "        return attributes\n",
    "    else:\n",
    "        return None # (if no attributes are given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (adapted from Jin)\n",
    "# equalize an OD for DIFFFERENT attributes in O and D\n",
    "# multiply baseline with the number of opportunities in the destination and with the attribute of i over the avg **-delta\n",
    "\n",
    "def equalization_all_2attributes(od, variable, colnameO, colnameD, delta, centroids): \n",
    "    \n",
    "    od_ = od.copy()\n",
    "    variable_ = variable.copy()\n",
    "    \n",
    "    variable_average1 = np.mean(variable_[colnameO])\n",
    "    \n",
    "    # calculate the attribute of i over the avg and **-delta \n",
    "    variable_['weightO'] = variable_[colnameO].apply(lambda x: (x / variable_average1) ** -delta)\n",
    "\n",
    "    # get the number of opportunities at j\n",
    "    variable_['weightD'] = variable_[colnameD]\n",
    "    \n",
    "    i = 0\n",
    "    for val in variable_['ig']:\n",
    "        weightO = variable_.loc[variable_['ig'] == val]['weightO'].iloc[0]\n",
    "        weightD = variable_.loc[variable_['ig'] == val]['weightD'].iloc[0]\n",
    "        try:\n",
    "            od_.loc[centroids.index(val)] *= weightO #row = origin\n",
    "            od_[centroids.index(val)] *= weightD #column = destination\n",
    "        except:\n",
    "            continue\n",
    "        i += 1\n",
    "    \n",
    "    return od_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function to use the function above in a batch\n",
    "def equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST1, COLOFINTEREST2, delta):\n",
    "    col_tokeep = ['osmid', 'ig', 'CODE_IRIS', COLOFINTEREST1, COLOFINTEREST2]\n",
    "    COLSOFINTEREST_df = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['centroid'] == True].copy()\n",
    "    COLSOFINTEREST_df = COLSOFINTEREST_df[col_tokeep]\n",
    "    \n",
    "    equalized_od = equalization_all_2attributes(baseline_df, COLSOFINTEREST_df, COLOFINTEREST1, COLOFINTEREST2, delta, centroids)\n",
    "    \n",
    "    equalized_od_name = \"OD_equalization_\" + COLOFINTEREST1 + \"_O_\"+ COLOFINTEREST2 + \"_D_delta_\" + str(delta)\n",
    "    \n",
    "    return {equalized_od_name: equalized_od}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Shapes\n",
    "\n",
    "# GPM outline\n",
    "GPM = gpd.read_file('data/raw/GPM.geojson').to_crs(crs_fr)\n",
    "\n",
    "# IRIS codes and shapes \n",
    "IRIS_GPM = gpd.read_file('data/raw/IRIS_GPM.geojson')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network in both NetworkX and igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create the network in NetworkX\n",
    "# Retrieve edges\n",
    "edges_with_id = pd.read_csv('data/clean/initial_network_edges.csv')\n",
    "edges_with_id[\"geometry\"] = edges_with_id.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "edges_with_id = gpd.GeoDataFrame(edges_with_id, geometry = 'geometry', crs = 4326).to_crs(2154)\n",
    "edges_with_id = edges_with_id.rename(columns={\"id\": \"G\"})\n",
    "\n",
    "# Retrieve nodes\n",
    "nodes_carbike_centroids_RER_complete = pd.read_csv('data/clean/initial_network_nodes_complete.csv')\n",
    "nodes_carbike_centroids_RER_complete[\"geometry\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "nodes_carbike_centroids_RER_complete = gpd.GeoDataFrame(nodes_carbike_centroids_RER_complete, geometry = 'geometry', crs = 2154)\n",
    "\n",
    "# Create the attr_dict\n",
    "nodes_carbike_centroids_RER_complete[\"attr_dict\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: make_attr_dict(\n",
    "                                                                  nodetype = x.nodetype,\n",
    "                                                                  centroid = x.centroid,\n",
    "                                                                  RER = x.RER,\n",
    "                                                                  IRIS = x.CODE_IRIS,\n",
    "                                                                  pop_dens = x.pop_density,\n",
    "                                                                  active_pop_density = x.active_pop_density,\n",
    "                                                                  school_pop_density = x.school_pop_density,\n",
    "                                                                  num_schools = x.school_count,\n",
    "                                                                  num_jobs = x.num_jobs,\n",
    "                                                                  ),\n",
    "                                                                  axis = 1) \n",
    "\n",
    "# Create Graph with all nodes and edges\n",
    "G = nx.from_pandas_edgelist(edges_with_id, source='x', target='y', edge_attr=True)\n",
    "G.add_nodes_from(nodes_carbike_centroids_RER_complete.loc[:,[\"osmid\", \"attr_dict\"]].itertuples(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- Moving from NetworkX to igraph\n",
    "g_igraph = ig.Graph()\n",
    "networkx_graph = G\n",
    "g_igraph = ig.Graph.from_networkx(networkx_graph)\n",
    "\n",
    "# eids: \"conversion table\" for edge ids from igraph to nx \n",
    "# eids_nx = [tuple(sorted(literal_eval(g_igraph.es(i)[\"edge_id\"][0]))) for i in range(len(g_igraph.es))]\n",
    "# eids_ig = [i for i in range(len(g_igraph.es))]\n",
    "# eids_conv = pd.DataFrame({\"nx\": eids_nx, \"ig\": eids_ig})\n",
    "\n",
    "#MOD made my own way\n",
    "eids_nx = [g_igraph.es[i][\"G\"] for i in range(len(g_igraph.es))]\n",
    "eids_ig = [i for i in range(len(g_igraph.es))]\n",
    "eids_conv = pd.DataFrame({\"nx\": eids_nx, \"ig\": eids_ig})\n",
    "\n",
    "# nids: \"conversion table\" for node ids from igraph to nx\n",
    "nids_nx = [g_igraph.vs(i)[\"_nx_name\"][0] for i in range(len(g_igraph.vs))]\n",
    "nids_ig = [i for i in range(len(g_igraph.vs))]\n",
    "nids_conv = pd.DataFrame({\"nx\": nids_nx, \"ig\": nids_ig})\n",
    "nids_conv['nx'] = nids_conv['nx'].astype(int)\n",
    "\n",
    "# combine the conversion table with nodes_carbike_centroids_RER_complete\n",
    "nodes_carbike_centroids_RER_complete = nodes_carbike_centroids_RER_complete.merge(nids_conv, left_on = \"osmid\", right_on = \"nx\", how = \"left\")\n",
    "nodes_carbike_centroids_RER_complete = nodes_carbike_centroids_RER_complete.drop(columns = [\"nx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate centroids\n",
    "\n",
    "from itertools import combinations\n",
    "seq = g_igraph.vs.select(centroid_eq = True)\n",
    "centroids = [v.index for v in seq]\n",
    "centroids = centroids[0:2] #for testing purposes #TODO \n",
    "node_combinations = list(combinations(centroids, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "- shortest path length\n",
    "- school population density in i * deterrence factor\n",
    "- active population density in i * deterrence factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Shortest path length \n",
    "\n",
    "def process_node(args):\n",
    "    start_node, end_node = args\n",
    "    global g_igraph\n",
    "    shortest_path_length = g_igraph.distances(source=start_node, target=end_node, weights='weight')[0][0]\n",
    "    return (start_node, end_node, shortest_path_length)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Number of processes (cores) to use for parallel processing\n",
    "    num_processes = mp.cpu_count()\n",
    "    global g_igraph\n",
    "\n",
    "    # Create a pool of processes\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Apply the function to each node combination using parallel processing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Create a dictionary to store the shortest path lengths\n",
    "    output = {}\n",
    "    for start_node, end_node, shortest_path_length in results:\n",
    "        if start_node not in output:\n",
    "            output[start_node] = {}\n",
    "        output[start_node][end_node] = shortest_path_length\n",
    "\n",
    "    # Create an empty adjacency matrix\n",
    "    matrix = np.zeros((len(centroids), len(centroids)))\n",
    "\n",
    "    # Fill the adjacency matrix with shortest path lengths\n",
    "    for i, start_node in enumerate(centroids):\n",
    "        for j, end_node in enumerate(centroids):\n",
    "            if start_node in output and end_node in output[start_node]:\n",
    "                matrix[i, j] = output[start_node][end_node]\n",
    "                matrix[j, i] = output[start_node][end_node]\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- School population density in i, deterrence factor\n",
    "\n",
    "def process_node(args):\n",
    "    global matrix\n",
    "    o, d = args\n",
    "    if o == d:\n",
    "        return (o, d, 0)\n",
    "    else:\n",
    "        normalized_dist = matrix[o][d] / matrix.max()\n",
    "        demand = (\n",
    "            (g_igraph.vs[centroids[o]]['school_pop_density'])\n",
    "            * dist_decay * np.exp(-1 * normalized_dist)\n",
    "        )\n",
    "        return (o, d, demand)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    baseline = np.zeros((len(centroids), len(centroids)))\n",
    "    maxtrips = 100\n",
    "    dist_decay = 1\n",
    "\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Create node combinations\n",
    "    node_combinations = [(o, d) for o in range(len(centroids)) for d in range(len(centroids))]\n",
    "\n",
    "    # Calculate demand for each node combination using multiprocessing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Update baseline matrix with calculated demand\n",
    "    for o, d, demand in results:\n",
    "        baseline[o][d] = demand\n",
    "\n",
    "    # Normalize the matrix to the number of maxtrips\n",
    "    baseline = ((baseline / baseline.max()) * maxtrips)\n",
    "\n",
    "    # Round up to ensure each journey is made at least once\n",
    "    baseline = np.ceil(baseline).astype(int)\n",
    "    baseline_schoolpopdens = pd.DataFrame(baseline)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Active population density in i, deterrence factor\n",
    "\n",
    "def process_node(args):\n",
    "    global matrix\n",
    "    o, d = args\n",
    "    if o == d:\n",
    "        return (o, d, 0)\n",
    "    else:\n",
    "        normalized_dist = matrix[o][d] / matrix.max()\n",
    "        demand = (\n",
    "            (g_igraph.vs[centroids[o]]['active_pop_density'])\n",
    "            * dist_decay * np.exp(-1 * normalized_dist)\n",
    "        )\n",
    "        return (o, d, demand)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    baseline = np.zeros((len(centroids), len(centroids)))\n",
    "    maxtrips = 100\n",
    "    dist_decay = 1\n",
    "\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Create node combinations\n",
    "    node_combinations = [(o, d) for o in range(len(centroids)) for d in range(len(centroids))]\n",
    "\n",
    "    # Calculate demand for each node combination using multiprocessing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Update baseline matrix with calculated demand\n",
    "    for o, d, demand in results:\n",
    "        baseline[o][d] = demand\n",
    "\n",
    "    # Normalize the matrix to the number of maxtrips\n",
    "    baseline = ((baseline / baseline.max()) * maxtrips)\n",
    "\n",
    "    # Round up to ensure each journey is made at least once\n",
    "    baseline = np.ceil(baseline).astype(int)\n",
    "    baseline_activepopdens = pd.DataFrame(baseline)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 1: No equalization yet, only appropriate population density and relevant POIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Multiply the columns of baseline_schoolpopdens with the number of schools in j\n",
    "\n",
    "def process_node(args):\n",
    "    global baseline_schoolpopdens\n",
    "    o, d = args\n",
    "    if o == d:\n",
    "        return (o, d, 0)\n",
    "    else:\n",
    "        demand = (baseline_schoolpopdens[o][d] * (g_igraph.vs[centroids[d]]['num_schools']))\n",
    "        return (o, d, demand)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    school_pop_dens_school_count_noEQ  = np.zeros((len(centroids), len(centroids)))\n",
    "\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Create node combinations\n",
    "    node_combinations = [(o, d) for o in range(len(centroids)) for d in range(len(centroids))]\n",
    "\n",
    "    # Calculate demand for each node combination using multiprocessing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Update baseline matrix with calculated demand\n",
    "    for o, d, demand in results:\n",
    "        school_pop_dens_school_count_noEQ[o][d] = demand\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Multiply the columns of baseline_activepopdens with the number of jobs in j\n",
    "\n",
    "def process_node(args):\n",
    "    global baseline_activepopdens\n",
    "    o, d = args\n",
    "    if o == d:\n",
    "        return (o, d, 0)\n",
    "    else:\n",
    "        demand = (baseline_activepopdens[o][d] * (g_igraph.vs[centroids[d]]['num_jobs']))\n",
    "        return (o, d, demand)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    active_pop_dens_job_count_noEQ  = np.zeros((len(centroids), len(centroids)))\n",
    "\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Create node combinations\n",
    "    node_combinations = [(o, d) for o in range(len(centroids)) for d in range(len(centroids))]\n",
    "\n",
    "    # Calculate demand for each node combination using multiprocessing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Update baseline matrix with calculated demand\n",
    "    for o, d, demand in results:\n",
    "        active_pop_dens_job_count_noEQ[o][d] = demand\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 2: Same as above but equalizing for median income (jobs) and education level (schools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Schools/education level/school pop density\n",
    "\n",
    "def process_combination(combination):\n",
    "    COLOFINTEREST1, COLOFINTEREST2 = combination\n",
    "    delta_list = [0.5, 1, 1.5]\n",
    "    results = {}\n",
    "    for delta in delta_list:\n",
    "        result = equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_schoolpopdens, centroids, COLOFINTEREST1, COLOFINTEREST2, delta)\n",
    "        results.update(result)\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    combinations = [['edu_level', 'school_count']]\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    Results = pool.map(process_combination, combinations)\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Jobs/median income/active pop density\n",
    "def process_combination(combination):\n",
    "    COLOFINTEREST1, COLOFINTEREST2 = combination\n",
    "    delta_list = [0.5, 1, 1.5]\n",
    "    results = {}\n",
    "    for delta in delta_list:\n",
    "        result = equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_activepopdens, centroids, COLOFINTEREST1, COLOFINTEREST2, delta)\n",
    "        results.update(result)\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    combinations = [['median_income', 'num_jobs']]\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    RResults = pool.map(process_combination, combinations)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign traffic flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create dataframe of edges compatible with frank_wolfe function\n",
    "\n",
    "# goal columns: edge name, source, target, free flow time, capacity, alpha, beta\n",
    "\n",
    "#check whether all IDs are \n",
    "g_df = nx.to_pandas_edgelist(G)\n",
    "\n",
    "# Create compatible edge names\n",
    "g_df['edge'] = g_df.index + 1\n",
    "g_df['edge'] = g_df['edge'].apply(lambda x: 'E'+ str(x).zfill(4))\n",
    "\n",
    "# Adding the columns we don't have from the NetworkX network\n",
    "g_df = g_df[['edge', 'source', 'target', 'length', 'geometry', 'G']]\n",
    "g_df['capacity'] = 1e10\n",
    "g_df['alpha'] = 0.15 #no idea how this is set\n",
    "g_df['beta'] = 4.0 #same here\n",
    "\n",
    "# Create compatible node names based on the osmIDs\n",
    "g_df['source'] = g_df['source'].apply(lambda x: 'N'+ str(x).zfill(5))\n",
    "g_df['target'] = g_df['target'].apply(lambda x: 'N'+ str(x).zfill(5))\n",
    "g_df.reset_index(inplace=True)\n",
    "\n",
    "# We have to explicitly say, and assume, that each link is a two-way road\n",
    "g_df2 = g_df.copy()\n",
    "g_df2['source'] = g_df['target']\n",
    "g_df2['target'] = g_df['source']\n",
    "g_df2['edge'] = g_df2.index + 1 + len(g_df)\n",
    "g_df2['edge'] = g_df2['edge'].apply(lambda x: 'E'+ str(x).zfill(4))\n",
    "g_df = pd.concat([g_df, g_df2])\n",
    "geoms = g_df[['edge', 'geometry', 'index']]\n",
    "\n",
    "# Clean-up\n",
    "g_df.drop(['geometry', 'index'], axis=1, inplace=True)\n",
    "\n",
    "# Correct order of columns\n",
    "g_df = g_df[['edge', 'source', 'target', 'length', 'capacity', 'alpha', 'beta', 'G']]\n",
    "\n",
    "g_df.to_csv('data/clean/network.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create network compatible with frank_wolfe function\n",
    "nt = Network('net')\n",
    "node = Vertex(\"a\")\n",
    "\n",
    "# Use the file created above\n",
    "with open(\"data/clean/network.csv\") as fo:\n",
    "    lines = fo.readlines()[1:]\n",
    "    for ln in lines:\n",
    "        eg = ln.split(',')\n",
    "        nt.add_edge(Edge(eg))\n",
    "\n",
    "\n",
    "nt.init_cost()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Make it a batch run\n",
    "\n",
    "# Gather all result OD matrices\n",
    "Results.extend(RResults)\n",
    "# Results.extend(school_pop_dens_school_count_noEQ)\n",
    "# Results.extend(active_pop_dens_job_count_noEQ)\n",
    "\n",
    "OD_matrix_names = []\n",
    "OD_matrix = []\n",
    "\n",
    "for result in Results:\n",
    "    OD_matrix_names.append(list(result.keys()))\n",
    "    OD_matrix.append(list(result.values()))\n",
    "\n",
    "OD_matrices_names = [item for sublist in OD_matrix_names for item in sublist]\n",
    "OD_matrices = [dataframe for sublist in OD_matrix for dataframe in sublist]\n",
    "\n",
    "# create dictionary of igraph ID to modified osmID\n",
    "centroid_igraph_to_mod_osmID = {}\n",
    "for i in range(len(centroids)):\n",
    "    centroid_igraph_to_mod_osmID[i] = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['ig'] == centroids[i]]['osmid'].apply(lambda x: 'N'+ (str(x) + '.0').zfill(5)).values[0]\n",
    "\n",
    "# Create dictionary of matrix names and dict index\n",
    "dict_index = {}\n",
    "for i in range(len(OD_matrices_names)):\n",
    "    dict_index[i] = OD_matrices_names[i]\n",
    "\n",
    "# run frank-wolfe on all of them \n",
    "dicts = []\n",
    "for name in OD_matrices_names:\n",
    "    vol2 = None\n",
    "\n",
    "    # Get OD matrix\n",
    "    OD = OD_matrices[OD_matrices_names.index(name)]\n",
    "\n",
    "    # Rename the columns and rows according to the modified osmID \n",
    "    OD = OD.rename(columns = {i : centroid_igraph_to_mod_osmID[i] for i in range(len(OD))}) #rename index of centroid as osmid of centroid\n",
    "    OD.index = OD.columns\n",
    "\n",
    "    # From all centroids to all centroids\n",
    "    origins = OD.columns\n",
    "    destinations = origins\n",
    "    \n",
    "    vol2 = frank_wolfe(nt, OD, origins, destinations)\n",
    "    dicts.append(vol2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute benefit metric for all gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m num_processes \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mcpu_count()\n\u001b[1;32m     48\u001b[0m pool \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mPool(processes\u001b[39m=\u001b[39mnum_processes)\n\u001b[0;32m---> 50\u001b[0m results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(process_chunk, mygaps)\n\u001b[1;32m     51\u001b[0m pool\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     52\u001b[0m pool\u001b[39m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/site-packages/multiprocess/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/site-packages/multiprocess/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/site-packages/multiprocess/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# def process_chunk(chunk):\n",
    "#         chunk['path'] = chunk['path'].apply(eval)\n",
    "#         for j in range(len(dicts)):\n",
    "#                 try:\n",
    "#                         chunk[\"B_star\"+str(j)] = chunk.apply(lambda x: \n",
    "#                                         np.sum([dicts[j][g_df.loc[g_df['id'] == i]['edge'].values[0]] * edge_lengths[i] for i in x.path]), \n",
    "#                                         axis=1)\n",
    "#                         chunk[\"B\"+str(j)] = chunk[\"B_star\"+str(j)] / chunk[\"length\"]\n",
    "#                 except:\n",
    "#                        continue\n",
    "#         try:            \n",
    "#                 rows_to_delete = chunk[['B'+str(j) for j in range(len(dicts))]].apply(lambda x: all(val == 0.000000 for val in x), axis=1)\n",
    "#                 chunk = chunk[~rows_to_delete]\n",
    "#         except:\n",
    "#                 pass\n",
    "#         print('processed one chunk!')\n",
    "#         return chunk\n",
    "\n",
    "### TEST, only one dict ###\n",
    "import ast\n",
    "\n",
    "def process_row(row, edge_lengths, dicts, g_df):\n",
    "    try:\n",
    "        row['path'] = ast.literal_eval(row['path'])\n",
    "        j = 0\n",
    "        #TODO if still fails, try row.path_nx put into G.edges instead of row.path\n",
    "        row[\"B_star\" + str(j)] = np.sum([dicts[j][g_df.loc[g_df['G'] == eids_conv_dict[i]]['edge'].values[0]] * edge_lengths[i] for i in row.path])\n",
    "        row[\"B\" + str(j)] = row[\"B_star\" + str(j)] / row[\"length\"]\n",
    "        return row\n",
    "    except Exception as e:\n",
    "        print(\"Exception occurred at index:\", row.name)\n",
    "        print(\"Exception message:\", str(e))\n",
    "        return None\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    processed_rows = [process_row(row, edge_lengths, dicts, g_df) for _, row in chunk.iterrows()]\n",
    "    processed_chunk = pd.DataFrame(processed_rows)\n",
    "    return processed_chunk\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = './data/clean/identified_gaps_under50.csv'\n",
    "    mygaps = pd.read_csv(file_path, chunksize=50000)\n",
    "\n",
    "    eids_conv_dict = eids_conv.set_index('ig')['nx'].to_dict()\n",
    "    edge_lengths = {i: g_igraph.es[i][\"length\"] for i in range(len(g_igraph.es))}\n",
    "\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    results = pool.map(process_chunk, mygaps)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Concatenate all the processed chunks back into a single DataFrame\n",
    "    final_result = pd.concat(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by each benefit metric and save in separate csvs\n",
    "for j in range(len(dicts)):\n",
    "    mygaps.sort_values(by=['B'+str(j)], ascending=False).head(1500).to_csv('./data/clean/mygaps_B'+str(j)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_index = pd.DataFrame.from_dict(dict_index, orient='index')\n",
    "dict_index.to_csv('data/clean/dict_index.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- Custom function\n",
    "# # equalize an OD for the same attribute in O and D\n",
    "# def equalization_all(od, variable, colname, delta, centroids): \n",
    "    \n",
    "#     od_ = od.copy()\n",
    "#     variable_ = variable.copy()\n",
    "    \n",
    "#     variable_average = np.mean(variable_[colname]) \n",
    "    \n",
    "#     variable_['weight'] = variable_[colname].apply(lambda x: (x/variable_average)**-delta)\n",
    "\n",
    "#     i =0\n",
    "#     for val in variable_['ig']:\n",
    "#         weight = variable_.loc[variable_['ig']==val]['weight'].iloc[0]\n",
    "#         try:\n",
    "#             od_[centroids.index(val)] *= weight \n",
    "#             od_.loc[centroids.index(val)] *= weight \n",
    "#         except:\n",
    "#             continue\n",
    "# #             print(val, ' not found')\n",
    "#         i +=1\n",
    "    \n",
    "#     return od_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- Custom function to use the function above in a batch\n",
    "# def clean_data_with_od_matrices(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST, delta):\n",
    "#     col_tokeep = ['osmid', 'ig', 'CODE_IRIS', COLOFINTEREST]\n",
    "#     COLOFINTEREST_df = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['centroid'] == True].copy()\n",
    "#     COLOFINTEREST_df = COLOFINTEREST_df[col_tokeep]\n",
    "    \n",
    "#     OD_equalization = equalization_all(baseline_df, COLOFINTEREST_df, COLOFINTEREST, delta, centroids)\n",
    "    \n",
    "#     OD_equalization_name = \"OD_equalization_\" + COLOFINTEREST + \"_\" + str(delta)\n",
    "    \n",
    "#     return {OD_equalization_name: OD_equalization}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 1: equalizing for median income, education level, number of schools and number of jobs SEPARATELY (and multiplying with the population density of j)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: population density of i and j and exponential term with normalised distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def process_node(args):\n",
    "#     global matrix\n",
    "#     o, d = args\n",
    "#     if o == d:\n",
    "#         return (o, d, 0)\n",
    "#     else:\n",
    "#         normalized_dist = matrix[o][d] / matrix.max()\n",
    "#         demand = (\n",
    "#             (g_igraph.vs[centroids[o]]['pop_dens'] * g_igraph.vs[centroids[d]]['pop_dens'])\n",
    "#             * dist_decay * np.exp(-1 * normalized_dist)\n",
    "#         )\n",
    "#         return (o, d, demand)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     baseline = np.zeros((len(centroids), len(centroids)))\n",
    "#     maxtrips = 100\n",
    "#     dist_decay = 1\n",
    "\n",
    "#     num_processes = mp.cpu_count()\n",
    "#     pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "#     # Create node combinations\n",
    "#     node_combinations = [(o, d) for o in range(len(centroids)) for d in range(len(centroids))]\n",
    "\n",
    "#     # Calculate demand for each node combination using multiprocessing\n",
    "#     results = pool.map(process_node, node_combinations)\n",
    "\n",
    "#     # Update baseline matrix with calculated demand\n",
    "#     for o, d, demand in results:\n",
    "#         baseline[o][d] = demand\n",
    "\n",
    "#     # Normalize the matrix to the number of maxtrips\n",
    "#     baseline = ((baseline / baseline.max()) * maxtrips)\n",
    "\n",
    "#     # Round up to ensure each journey is made at least once\n",
    "#     baseline = np.ceil(baseline).astype(int)\n",
    "#     baseline_df1 = pd.DataFrame(baseline)\n",
    "\n",
    "#     pool.close()\n",
    "#     pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the equalisation code like in Jin's paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# def process_data(args):\n",
    "#     COLOFINTEREST = args\n",
    "#     delta_list = [0.5, 1, 1.5]\n",
    "#     results = {}\n",
    "#     for delta in delta_list:\n",
    "#         result = clean_data_with_od_matrices(nodes_carbike_centroids_RER_complete, baseline_df1, centroids, COLOFINTEREST, delta)\n",
    "#         results.update(result)\n",
    "#     return results\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     num_processes = mp.cpu_count()\n",
    "\n",
    "#     # Create a pool of processes\n",
    "#     pool = mp.Pool(processes=num_processes)    \n",
    "#     COLOFINTEREST_list = ['median_income', 'school_count', 'num_jobs', 'edu_level']\n",
    "#     arguments = [COLOFINTEREST for COLOFINTEREST in COLOFINTEREST_list]\n",
    "#     results = pool.map(process_data, arguments)\n",
    "#     pool.close()\n",
    "#     pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[0]['OD_equalization_median_income_1.5']\n",
    "# results_JinEQ = results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 2: equalize for median income/ education level when looking at number of jobs/number of schools, use total pop density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- Total Population density at origin, num_jobs and school_count at destination, Median income and edu level at origin\n",
    "# def process_combination(combination):\n",
    "#     COLOFINTEREST1, COLOFINTEREST2 = combination\n",
    "#     delta_list = [0.5, 1, 1.5]\n",
    "#     results = {}\n",
    "#     for delta in delta_list:\n",
    "#         result = equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_df2, centroids, COLOFINTEREST1, COLOFINTEREST2, delta)\n",
    "#         results.update(result)\n",
    "#     return results\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     combinations = [['median_income', 'num_jobs'], ['edu_level', 'school_count']]\n",
    "#     num_processes = mp.cpu_count()\n",
    "#     pool = mp.Pool(processes=num_processes)\n",
    "#     Results = pool.map(process_combination, combinations)\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "# for i in range(len(Results)):\n",
    "#     for old_key in Results[i]:\n",
    "#         Results[i][old_key+'_totpopdens'] = Results[i].pop(old_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
