{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the OD Matrices\n",
    "- Matrix 0: shortest trips between centroids\n",
    "- Baseline: pop density and exp(normalized distance) -> gravity model baseline like Yap et al.\n",
    "- Matrix set 1: equalizing for median income, education level, number of schools and number of jobs SEPARATELY\n",
    "- Matrix set 2: equalizing for different attributes in O and D. O/D equalized for education level/number of schools, median income/number of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import osmnx as nx\n",
    "import shapely\n",
    "import multiprocess as mp\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "import igraph as ig\n",
    "\n",
    "crs_fr = 2154\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (Anastassia)\n",
    "%run -i packages.py\n",
    "def make_attr_dict(*args, **kwargs): \n",
    "    \n",
    "    argCount = len(kwargs)\n",
    "    \n",
    "    if argCount > 0:\n",
    "        attributes = {}\n",
    "        for kwarg in kwargs:\n",
    "            attributes[kwarg] = kwargs.get(kwarg, None)\n",
    "        return attributes\n",
    "    else:\n",
    "        return None # (if no attributes are given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (Jin)\n",
    "# equalize an OD for the same attribute in O and D\n",
    "def equalization_all(od, variable, colname, delta, centroids): \n",
    "    \n",
    "    od_ = od.copy()\n",
    "    variable_ = variable.copy()\n",
    "    \n",
    "    variable_average = np.mean(variable_[colname]) \n",
    "    \n",
    "    variable_['weight'] = variable_[colname].apply(lambda x: (x/variable_average)**-delta)\n",
    "\n",
    "    i =0\n",
    "    for val in variable_['ig']:\n",
    "        weight = variable_.loc[variable_['ig']==val]['weight'].iloc[0]\n",
    "        try:\n",
    "            od_[centroids.index(val)] *= weight \n",
    "            od_.loc[centroids.index(val)] *= weight \n",
    "        except:\n",
    "            continue\n",
    "#             print(val, ' not found')\n",
    "        i +=1\n",
    "    \n",
    "    return od_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function to use the function above in a batch\n",
    "def clean_data_with_od_matrices(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST, delta):\n",
    "    col_tokeep = ['osmid', 'ig', 'CODE_IRIS', COLOFINTEREST]\n",
    "    COLOFINTEREST_df = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['centroid'] == True].copy()\n",
    "    COLOFINTEREST_df = COLOFINTEREST_df[col_tokeep]\n",
    "    \n",
    "    OD_equalization = equalization_all(baseline_df, COLOFINTEREST_df, COLOFINTEREST, delta, centroids)\n",
    "    \n",
    "    OD_equalization_name = \"OD_equalization_\" + COLOFINTEREST + \"_\" + str(delta)\n",
    "    \n",
    "    return {OD_equalization_name: OD_equalization}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--- Custom function (adapted from Jin)\n",
    "# # equalize an OD for DIFFFERENT attributes in O and D\n",
    "\n",
    "def equalization_all_2attributes(od, variable, colnameO, colnameD, delta, centroids): \n",
    "    \n",
    "    od_ = od.copy()\n",
    "    variable_ = variable.copy()\n",
    "    \n",
    "    variable_average1 = np.mean(variable_[colnameO])\n",
    "    variable_average2 = np.mean(variable_[colnameD])\n",
    "    \n",
    "    #here we keep -delta because we want to penalize the high values\n",
    "    # e.g low income is prioritized \n",
    "    variable_['weightO'] = variable_[colnameO].apply(lambda x: (x / variable_average1) ** -delta)\n",
    "\n",
    "    # here we use +delta because we want to penalize the low values\n",
    "    # e.g high number of jobs in an area is prioritzed\n",
    "    variable_['weightD'] = variable_[colnameD].apply(lambda x: (x / variable_average2) ** delta) #\n",
    "    \n",
    "    i = 0\n",
    "    for val in variable_['ig']:\n",
    "        weightO = variable_.loc[variable_['ig'] == val]['weightO'].iloc[0]\n",
    "        weightD = variable_.loc[variable_['ig'] == val]['weightD'].iloc[0]\n",
    "        try:\n",
    "            od_.loc[centroids.index(val)] *= weightO #row = origin\n",
    "            od_[centroids.index(val)] *= weightD #column = destination\n",
    "        except:\n",
    "            continue\n",
    "        i += 1\n",
    "    \n",
    "    return od_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function to use the function above in a batch\n",
    "def equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST1, COLOFINTEREST2, delta):\n",
    "    col_tokeep = ['osmid', 'ig', 'CODE_IRIS', COLOFINTEREST1, COLOFINTEREST2]\n",
    "    COLSOFINTEREST_df = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['centroid'] == True].copy()\n",
    "    COLSOFINTEREST_df = COLSOFINTEREST_df[col_tokeep]\n",
    "    \n",
    "    equalized_od = equalization_all_2attributes(baseline_df, COLSOFINTEREST_df, COLOFINTEREST1, COLOFINTEREST2, delta, centroids)\n",
    "    \n",
    "    equalized_od_name = \"OD_equalization_\" + COLOFINTEREST1 + \"_O_\"+ COLOFINTEREST2 + \"_D_delta_\" + str(delta)\n",
    "    \n",
    "    return {equalized_od_name: equalized_od}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Shapes\n",
    "\n",
    "# GPM outline\n",
    "GPM = gpd.read_file('data/raw/GPM.geojson').to_crs(crs_fr)\n",
    "\n",
    "# IRIS codes and shapes \n",
    "IRIS_GPM = gpd.read_file('data/raw/IRIS_GPM.geojson')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network and adding igraph IDs to the node table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create the network in NetworkX\n",
    "# Retrieve edges\n",
    "edges_with_id = pd.read_csv('data/clean/initial_network_edges.csv')\n",
    "edges_with_id[\"geometry\"] = edges_with_id.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "edges_with_id = gpd.GeoDataFrame(edges_with_id, geometry = 'geometry', crs = 4326).to_crs(2154)\n",
    "\n",
    "# Retrieve nodes\n",
    "nodes_carbike_centroids_RER_complete = pd.read_csv('data/clean/initial_network_nodes_complete.csv')\n",
    "nodes_carbike_centroids_RER_complete[\"geometry\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "nodes_carbike_centroids_RER_complete = gpd.GeoDataFrame(nodes_carbike_centroids_RER_complete, geometry = 'geometry', crs = 2154)\n",
    "\n",
    "# Create the attr_dict\n",
    "nodes_carbike_centroids_RER_complete[\"attr_dict\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: make_attr_dict(\n",
    "                                                                  nodetype = x.nodetype,\n",
    "                                                                  centroid = x.centroid,\n",
    "                                                                  RER = x.RER,\n",
    "                                                                  IRIS = x.CODE_IRIS,\n",
    "                                                                  pop_dens = x.pop_density,\n",
    "                                                                  active_pop_density = x.active_pop_density,\n",
    "                                                                  school_pop_density = x.school_pop_density,\n",
    "                                                                  school_count = x.school_count,\n",
    "                                                                  num_jobs = x.num_jobs,\n",
    "                                                                  ),\n",
    "                                                                  axis = 1) \n",
    "\n",
    "# Create Graph with all nodes and edges\n",
    "G = nx.from_pandas_edgelist(edges_with_id, source='x', target='y', edge_attr=True)\n",
    "G.add_nodes_from(nodes_carbike_centroids_RER_complete.loc[:,[\"osmid\", \"attr_dict\"]].itertuples(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Moving from NetworkX to igraph\n",
    "g_igraph = ig.Graph()\n",
    "networkx_graph = G\n",
    "g_igraph = ig.Graph.from_networkx(networkx_graph)\n",
    "\n",
    "# eids: \"conversion table\" for edge ids from igraph to nx \n",
    "eids_nx = [tuple(sorted(literal_eval(g_igraph.es(i)[\"edge_id\"][0]))) for i in range(len(g_igraph.es))]\n",
    "eids_ig = [i for i in range(len(g_igraph.es))]\n",
    "eids_conv = pd.DataFrame({\"nx\": eids_nx, \"ig\": eids_ig})\n",
    "\n",
    "# nids: \"conversion table\" for node ids from igraph to nx\n",
    "nids_nx = [g_igraph.vs(i)[\"_nx_name\"][0] for i in range(len(g_igraph.vs))]\n",
    "nids_ig = [i for i in range(len(g_igraph.vs))]\n",
    "nids_conv = pd.DataFrame({\"nx\": nids_nx, \"ig\": nids_ig})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nids_conv['nx'] = nids_conv['nx'].astype(int)\n",
    "\n",
    "# combine the conversion table with nodes_carbike_centroids_RER_complete\n",
    "nodes_carbike_centroids_RER_complete = nodes_carbike_centroids_RER_complete.merge(nids_conv, left_on = \"osmid\", right_on = \"nx\", how = \"left\")\n",
    "nodes_carbike_centroids_RER_complete = nodes_carbike_centroids_RER_complete.drop(columns = [\"nx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate centroids\n",
    "from itertools import combinations\n",
    "seq = g_igraph.vs.select(centroid_eq = True)\n",
    "centroids = [v.index for v in seq]\n",
    "centroids = centroids[0:2] #for testing purposes \n",
    "node_combinations = list(combinations(centroids, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix 0: shortest path between each pair of centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2m/lkz691q11b72bpprh6_v34_h0000gn/T/ipykernel_21954/1986734839.py:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "  shortest_path_length = g_igraph.shortest_paths_dijkstra(source=start_node, target=end_node, weights='weight')[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "Time:  28.87299732500105\n"
     ]
    }
   ],
   "source": [
    "# Create OD matrix\n",
    "def process_node(args):\n",
    "    start_node, end_node = args\n",
    "    global g_igraph\n",
    "    shortest_path_length = g_igraph.shortest_paths_dijkstra(source=start_node, target=end_node, weights='weight')[0][0]\n",
    "    return (start_node, end_node, shortest_path_length)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Number of processes (cores) to use for parallel processing\n",
    "    num_processes = 4\n",
    "    global g_igraph\n",
    "\n",
    "    # Create a pool of processes\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Apply the function to each node combination using parallel processing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Create a dictionary to store the shortest path lengths\n",
    "    output = {}\n",
    "    for start_node, end_node, shortest_path_length in results:\n",
    "        if start_node not in output:\n",
    "            output[start_node] = {}\n",
    "        output[start_node][end_node] = shortest_path_length\n",
    "\n",
    "    # Create an empty adjacency matrix\n",
    "    matrix = np.zeros((len(centroids), len(centroids)))\n",
    "\n",
    "    # Fill the adjacency matrix with shortest path lengths\n",
    "    for i, start_node in enumerate(centroids):\n",
    "        for j, end_node in enumerate(centroids):\n",
    "            if start_node in output and end_node in output[start_node]:\n",
    "                matrix[i, j] = output[start_node][end_node]\n",
    "                matrix[j, i] = output[start_node][end_node]\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(matrix.shape)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: population densities and exponential term with normalised distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate demand between each origin and destination\n",
    "# NO MULTIPROCESSING\n",
    "baseline = np.zeros((len(centroids), len(centroids)))\n",
    "maxtrips = 100\n",
    "dist_decay = 1\n",
    "\n",
    "for o in range(0, len(centroids)):\n",
    "    for d in range(0, len(centroids)):\n",
    "        if o == d:\n",
    "            # do not insert demand down the spine - no trips where origin = destination\n",
    "            baseline[o][d] = 0\n",
    "        else:\n",
    "            # normalize the current travel time versus the largest travel time between nodes in the matrix\n",
    "            normalized_dist = matrix[o][d] / matrix.max()\n",
    "\n",
    "            #  here, demand is a function of the product of the population of the origin and\n",
    "            #  the destination - but reduced by the distance between them. 'Gravity demand'\n",
    "            baseline[o][d] = ((g_igraph.vs[centroids[o]]['pop_dens'] * g_igraph.vs[centroids[d]]['pop_dens']) * dist_decay * np.exp(-1 * normalized_dist))\n",
    "\n",
    "# we normalize the matrix to the number of maxtrips\n",
    "baseline = ((baseline / baseline.max()) * maxtrips)\n",
    "\n",
    "# we round up - to ensure each journey is made at least once\n",
    "baseline = np.ceil(baseline).astype(int)\n",
    "baseline_df = pd.DataFrame(baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0    0  100\n",
       "1  100    0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 1: equalizing for median income, education level, number of schools and number of jobs SEPARATELY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for COLOFINTEREST in ['median_income', 'school_count', 'num_jobs']:\n",
    "    for delta in (0.5, 1, 1.5):\n",
    "        result = clean_data_with_od_matrices(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST, delta)\n",
    "        results.append(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 2: equalize for O/D attributes median income/ number of jobs, education level/number of schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'OD_equalization_median_income_O_num_jobs_D_delta_0.5':            0           1\n",
       "  0   0.000000  139.714224\n",
       "  1  52.024671    0.000000},\n",
       " {'OD_equalization_median_income_O_num_jobs_D_delta_1':            0           1\n",
       "  0   0.000000  195.200644\n",
       "  1  27.065664    0.000000},\n",
       " {'OD_equalization_median_income_O_num_jobs_D_delta_1.5':            0           1\n",
       "  0   0.000000  272.723065\n",
       "  1  14.080823    0.000000}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinations = [['median_income', 'num_jobs']]#,['edu_level', 'num_schools']]\n",
    "Results = []\n",
    "for combination in combinations:\n",
    "    for delta in (0.5, 1, 1.5):\n",
    "        result = equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_df, centroids, combination[0], combination[1], delta)\n",
    "        Results.append(result)\n",
    "\n",
    "Results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
