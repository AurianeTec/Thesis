{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the OD Matrices\n",
    "- Matrix 0: shortest trips between centroids\n",
    "- Baseline: pop density and exp(normalized distance) -> gravity model baseline like Yap et al.\n",
    "- Matrix set 1: equalizing for median income, education level, number of schools and number of jobs SEPARATELY\n",
    "- Matrix set 2: equalizing for different attributes in O and D. O/D equalized for education level/number of schools, median income/number of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import osmnx as nx\n",
    "import shapely\n",
    "import multiprocess as mp\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "import igraph as ig\n",
    "from ta_lab.assignment.assign import frank_wolfe\n",
    "from ta_lab.assignment.line import *\n",
    "from ta_lab.assignment.graph import *\n",
    "from ta_lab.assignment.shortest_path import ShortestPath as SPP\n",
    "\n",
    "crs_fr = 2154\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (Anastassia)\n",
    "# Create a dictionary of attributes (useful for networkX)\n",
    "%run -i packages.py\n",
    "def make_attr_dict(*args, **kwargs): \n",
    "    \n",
    "    argCount = len(kwargs)\n",
    "    \n",
    "    if argCount > 0:\n",
    "        attributes = {}\n",
    "        for kwarg in kwargs:\n",
    "            attributes[kwarg] = kwargs.get(kwarg, None)\n",
    "        return attributes\n",
    "    else:\n",
    "        return None # (if no attributes are given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (Jin)\n",
    "# equalize an OD for the same attribute in O and D\n",
    "def equalization_all(od, variable, colname, delta, centroids): \n",
    "    \n",
    "    od_ = od.copy()\n",
    "    variable_ = variable.copy()\n",
    "    \n",
    "    variable_average = np.mean(variable_[colname]) \n",
    "    \n",
    "    variable_['weight'] = variable_[colname].apply(lambda x: (x/variable_average)**-delta)\n",
    "\n",
    "    i =0\n",
    "    for val in variable_['ig']:\n",
    "        weight = variable_.loc[variable_['ig']==val]['weight'].iloc[0]\n",
    "        try:\n",
    "            od_[centroids.index(val)] *= weight \n",
    "            od_.loc[centroids.index(val)] *= weight \n",
    "        except:\n",
    "            continue\n",
    "#             print(val, ' not found')\n",
    "        i +=1\n",
    "    \n",
    "    return od_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function to use the function above in a batch\n",
    "def clean_data_with_od_matrices(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST, delta):\n",
    "    col_tokeep = ['osmid', 'ig', 'CODE_IRIS', COLOFINTEREST]\n",
    "    COLOFINTEREST_df = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['centroid'] == True].copy()\n",
    "    COLOFINTEREST_df = COLOFINTEREST_df[col_tokeep]\n",
    "    \n",
    "    OD_equalization = equalization_all(baseline_df, COLOFINTEREST_df, COLOFINTEREST, delta, centroids)\n",
    "    \n",
    "    OD_equalization_name = \"OD_equalization_\" + COLOFINTEREST + \"_\" + str(delta)\n",
    "    \n",
    "    return {OD_equalization_name: OD_equalization}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (adapted from Jin)\n",
    "# equalize an OD for DIFFFERENT attributes in O and D\n",
    "\n",
    "def equalization_all_2attributes(od, variable, colnameO, colnameD, delta, centroids): \n",
    "    \n",
    "    od_ = od.copy()\n",
    "    variable_ = variable.copy()\n",
    "    \n",
    "    variable_average1 = np.mean(variable_[colnameO])\n",
    "    variable_average2 = np.mean(variable_[colnameD])\n",
    "    \n",
    "    #here we keep -delta because we want to penalize the high values\n",
    "    # e.g low income is prioritized \n",
    "    variable_['weightO'] = variable_[colnameO].apply(lambda x: (x / variable_average1) ** -delta)\n",
    "\n",
    "    # here we use +delta because we want to penalize the low values\n",
    "    # e.g high number of jobs in an area is prioritzed\n",
    "    variable_['weightD'] = variable_[colnameD].apply(lambda x: (x / variable_average2) ** delta) #\n",
    "    \n",
    "    i = 0\n",
    "    for val in variable_['ig']:\n",
    "        weightO = variable_.loc[variable_['ig'] == val]['weightO'].iloc[0]\n",
    "        weightD = variable_.loc[variable_['ig'] == val]['weightD'].iloc[0]\n",
    "        try:\n",
    "            od_.loc[centroids.index(val)] *= weightO #row = origin\n",
    "            od_[centroids.index(val)] *= weightD #column = destination\n",
    "        except:\n",
    "            continue\n",
    "        i += 1\n",
    "    \n",
    "    return od_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function to use the function above in a batch\n",
    "def equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST1, COLOFINTEREST2, delta):\n",
    "    col_tokeep = ['osmid', 'ig', 'CODE_IRIS', COLOFINTEREST1, COLOFINTEREST2]\n",
    "    COLSOFINTEREST_df = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['centroid'] == True].copy()\n",
    "    COLSOFINTEREST_df = COLSOFINTEREST_df[col_tokeep]\n",
    "    \n",
    "    equalized_od = equalization_all_2attributes(baseline_df, COLSOFINTEREST_df, COLOFINTEREST1, COLOFINTEREST2, delta, centroids)\n",
    "    \n",
    "    equalized_od_name = \"OD_equalization_\" + COLOFINTEREST1 + \"_O_\"+ COLOFINTEREST2 + \"_D_delta_\" + str(delta)\n",
    "    \n",
    "    return {equalized_od_name: equalized_od}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Shapes\n",
    "\n",
    "# GPM outline\n",
    "GPM = gpd.read_file('data/raw/GPM.geojson').to_crs(crs_fr)\n",
    "\n",
    "# IRIS codes and shapes \n",
    "IRIS_GPM = gpd.read_file('data/raw/IRIS_GPM.geojson')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network in both NetworkX and igraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create the network in NetworkX\n",
    "# Retrieve edges\n",
    "edges_with_id = pd.read_csv('data/clean/initial_network_edges_complete.csv')\n",
    "edges_with_id[\"geometry\"] = edges_with_id.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "edges_with_id = gpd.GeoDataFrame(edges_with_id, geometry = 'geometry', crs = 4326).to_crs(2154)\n",
    "\n",
    "# Retrieve nodes\n",
    "nodes_carbike_centroids_RER_complete = pd.read_csv('data/clean/initial_network_nodes_complete.csv')\n",
    "nodes_carbike_centroids_RER_complete[\"geometry\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "nodes_carbike_centroids_RER_complete = gpd.GeoDataFrame(nodes_carbike_centroids_RER_complete, geometry = 'geometry', crs = 2154)\n",
    "\n",
    "# Create the attr_dict\n",
    "nodes_carbike_centroids_RER_complete[\"attr_dict\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: make_attr_dict(\n",
    "                                                                  nodetype = x.nodetype,\n",
    "                                                                  centroid = x.centroid,\n",
    "                                                                  RER = x.RER,\n",
    "                                                                  IRIS = x.CODE_IRIS,\n",
    "                                                                  pop_dens = x.pop_density,\n",
    "                                                                  active_pop_density = x.active_pop_density,\n",
    "                                                                  school_pop_density = x.school_pop_density,\n",
    "                                                                  num_schools = x.school_count,\n",
    "                                                                  num_jobs = x.num_jobs,\n",
    "                                                                  ),\n",
    "                                                                  axis = 1) \n",
    "\n",
    "# Create Graph with all nodes and edges\n",
    "G = nx.from_pandas_edgelist(edges_with_id, source='x', target='y', edge_attr=True)\n",
    "G.add_nodes_from(nodes_carbike_centroids_RER_complete.loc[:,[\"osmid\", \"attr_dict\"]].itertuples(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Moving from NetworkX to igraph\n",
    "g_igraph = ig.Graph()\n",
    "networkx_graph = G\n",
    "g_igraph = ig.Graph.from_networkx(networkx_graph)\n",
    "\n",
    "# eids: \"conversion table\" for edge ids from igraph to nx \n",
    "eids_nx = [tuple(sorted(literal_eval(g_igraph.es(i)[\"edge_id\"][0]))) for i in range(len(g_igraph.es))]\n",
    "eids_ig = [i for i in range(len(g_igraph.es))]\n",
    "eids_conv = pd.DataFrame({\"nx\": eids_nx, \"ig\": eids_ig})\n",
    "\n",
    "# nids: \"conversion table\" for node ids from igraph to nx\n",
    "nids_nx = [g_igraph.vs(i)[\"_nx_name\"][0] for i in range(len(g_igraph.vs))]\n",
    "nids_ig = [i for i in range(len(g_igraph.vs))]\n",
    "nids_conv = pd.DataFrame({\"nx\": nids_nx, \"ig\": nids_ig})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nids_conv['nx'] = nids_conv['nx'].astype(int)\n",
    "\n",
    "# combine the conversion table with nodes_carbike_centroids_RER_complete\n",
    "nodes_carbike_centroids_RER_complete = nodes_carbike_centroids_RER_complete.merge(nids_conv, left_on = \"osmid\", right_on = \"nx\", how = \"left\")\n",
    "nodes_carbike_centroids_RER_complete = nodes_carbike_centroids_RER_complete.drop(columns = [\"nx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate centroids\n",
    "from itertools import combinations\n",
    "seq = g_igraph.vs.select(centroid_eq = True)\n",
    "centroids = [v.index for v in seq]\n",
    "centroids = centroids[0:2] #for testing purposes \n",
    "node_combinations = list(combinations(centroids, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix 0: shortest path between each pair of centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "CPU times: user 22.2 ms, sys: 41 ms, total: 63.2 ms\n",
      "Wall time: 259 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create OD matrix\n",
    "def process_node(args):\n",
    "    start_node, end_node = args\n",
    "    global g_igraph\n",
    "    shortest_path_length = g_igraph.shortest_paths_dijkstra(source=start_node, target=end_node, weights='weight')[0][0]\n",
    "    return (start_node, end_node, shortest_path_length)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Number of processes (cores) to use for parallel processing\n",
    "    num_processes = mp.cpu_count()\n",
    "    global g_igraph\n",
    "\n",
    "    # Create a pool of processes\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Apply the function to each node combination using parallel processing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Create a dictionary to store the shortest path lengths\n",
    "    output = {}\n",
    "    for start_node, end_node, shortest_path_length in results:\n",
    "        if start_node not in output:\n",
    "            output[start_node] = {}\n",
    "        output[start_node][end_node] = shortest_path_length\n",
    "\n",
    "    # Create an empty adjacency matrix\n",
    "    matrix = np.zeros((len(centroids), len(centroids)))\n",
    "\n",
    "    # Fill the adjacency matrix with shortest path lengths\n",
    "    for i, start_node in enumerate(centroids):\n",
    "        for j, end_node in enumerate(centroids):\n",
    "            if start_node in output and end_node in output[start_node]:\n",
    "                matrix[i, j] = output[start_node][end_node]\n",
    "                matrix[j, i] = output[start_node][end_node]\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(matrix.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: population densities and exponential term with normalised distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 ms, sys: 34.6 ms, total: 52.4 ms\n",
      "Wall time: 60.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_node(args):\n",
    "    global matrix\n",
    "    o, d = args\n",
    "    if o == d:\n",
    "        return (o, d, 0)\n",
    "    else:\n",
    "        normalized_dist = matrix[o][d] / matrix.max()\n",
    "        demand = (\n",
    "            (g_igraph.vs[centroids[o]]['pop_dens'] * g_igraph.vs[centroids[d]]['pop_dens'])\n",
    "            * dist_decay * np.exp(-1 * normalized_dist)\n",
    "        )\n",
    "        return (o, d, demand)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    baseline = np.zeros((len(centroids), len(centroids)))\n",
    "    maxtrips = 100\n",
    "    dist_decay = 1\n",
    "\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Create node combinations\n",
    "    node_combinations = [(o, d) for o in range(len(centroids)) for d in range(len(centroids))]\n",
    "\n",
    "    # Calculate demand for each node combination using multiprocessing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Update baseline matrix with calculated demand\n",
    "    for o, d, demand in results:\n",
    "        baseline[o][d] = demand\n",
    "\n",
    "    # Normalize the matrix to the number of maxtrips\n",
    "    baseline = ((baseline / baseline.max()) * maxtrips)\n",
    "\n",
    "    # Round up to ensure each journey is made at least once\n",
    "    baseline = np.ceil(baseline).astype(int)\n",
    "    baseline_df = pd.DataFrame(baseline)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 276 µs, sys: 177 µs, total: 453 µs\n",
      "Wall time: 450 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate demand between each origin and destination\n",
    "# NO MULTIPROCESSING\n",
    "baseline = np.zeros((len(centroids), len(centroids)))\n",
    "maxtrips = 100\n",
    "dist_decay = 1\n",
    "\n",
    "for o in range(0, len(centroids)):\n",
    "    for d in range(0, len(centroids)):\n",
    "        if o == d:\n",
    "            # do not insert demand down the spine - no trips where origin = destination\n",
    "            baseline[o][d] = 0\n",
    "        else:\n",
    "            # normalize the current travel time versus the largest travel time between nodes in the matrix\n",
    "            normalized_dist = matrix[o][d] / matrix.max()\n",
    "\n",
    "            #  here, demand is a function of the product of the population of the origin and\n",
    "            #  the destination - but reduced by the distance between them. 'Gravity demand'\n",
    "            baseline[o][d] = ((g_igraph.vs[centroids[o]]['pop_dens'] * g_igraph.vs[centroids[d]]['pop_dens']) * dist_decay * np.exp(-1 * normalized_dist))\n",
    "\n",
    "# we normalize the matrix to the number of maxtrips\n",
    "baseline = ((baseline / baseline.max()) * maxtrips)\n",
    "\n",
    "# we round up - to ensure each journey is made at least once\n",
    "baseline = np.ceil(baseline).astype(int)\n",
    "baseline_df = pd.DataFrame(baseline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 1: equalizing for median income, education level, number of schools and number of jobs SEPARATELY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 ms, sys: 36.2 ms, total: 56 ms\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_data(args):\n",
    "    COLOFINTEREST = args\n",
    "    delta_list = [0.5, 1, 1.5]\n",
    "    results = {}\n",
    "    for delta in delta_list:\n",
    "        result = clean_data_with_od_matrices(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST, delta)\n",
    "        results.update(result)\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_processes = mp.cpu_count()\n",
    "\n",
    "    # Create a pool of processes\n",
    "    pool = mp.Pool(processes=num_processes)    \n",
    "    COLOFINTEREST_list = ['median_income', 'school_count', 'num_jobs']\n",
    "    arguments = [COLOFINTEREST for COLOFINTEREST in COLOFINTEREST_list]\n",
    "    results = pool.map(process_data, arguments)\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 2: equalize for O/D attributes median income/ number of jobs, education level/number of schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 ms, sys: 36.4 ms, total: 54.8 ms\n",
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def process_combination(combination):\n",
    "    COLOFINTEREST1, COLOFINTEREST2 = combination\n",
    "    delta_list = [0.5, 1, 1.5]\n",
    "    results = {}\n",
    "    for delta in delta_list:\n",
    "        result = equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST1, COLOFINTEREST2, delta)\n",
    "        results.update(result)\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    combinations = [['median_income', 'num_jobs'], ['edu_level', 'school_count']]\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    Results = pool.map(process_combination, combinations)\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  38.85781563299679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6476302605499465"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)\n",
    "(stop - start)/60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign traffic flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create dataframe of edges compatible with frank_wolfe function\n",
    "\n",
    "# goal columns: edge name, source, target, free flow time, capacity, alpha, beta\n",
    "\n",
    "g_df = nx.to_pandas_edgelist(G)\n",
    "\n",
    "# Create compatible edge names\n",
    "g_df['edge'] = g_df.index + 1\n",
    "g_df['edge'] = g_df['edge'].apply(lambda x: 'E'+ str(x).zfill(4))\n",
    "\n",
    "g_df\n",
    "\n",
    "# Adding the columns we don't have from the NetworkX network\n",
    "g_df = g_df[['edge', 'source', 'target', 'length', 'geometry', 'id']]\n",
    "g_df['capacity'] = 1e15\n",
    "g_df['alpha'] = 0.15 #no idea how this is set\n",
    "g_df['beta'] = 4.0 #same here\n",
    "\n",
    "# Create compatible node names based on the osmIDs\n",
    "g_df['source'] = g_df['source'].apply(lambda x: 'N'+ str(x).zfill(5))\n",
    "g_df['target'] = g_df['target'].apply(lambda x: 'N'+ str(x).zfill(5))\n",
    "g_df.reset_index(inplace=True)\n",
    "\n",
    "# We have to explicitly say, and assume, that each link is a two-way road\n",
    "g_df2 = g_df.copy()\n",
    "g_df2['source'] = g_df['target']\n",
    "g_df2['target'] = g_df['source']\n",
    "g_df2['edge'] = g_df2.index + 1 + len(g_df)\n",
    "g_df2['edge'] = g_df2['edge'].apply(lambda x: 'E'+ str(x).zfill(4))\n",
    "g_df = pd.concat([g_df, g_df2])\n",
    "geoms = g_df[['edge', 'geometry', 'index']]\n",
    "\n",
    "# Clean-up\n",
    "g_df.drop(['geometry', 'index'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Save to csv\n",
    "g_df.to_csv(\"data/clean/test_network.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create network compatible with frank_wolfe function\n",
    "nt = Network('net')\n",
    "node = Vertex(\"a\")\n",
    "\n",
    "# Use the file created above\n",
    "with open(\"data/clean/test_network.csv\") as fo:\n",
    "    lines = fo.readlines()[1:]\n",
    "    for ln in lines:\n",
    "        eg = ln.split(',')\n",
    "        nt.add_edge(Edge(eg))\n",
    "\n",
    "\n",
    "nt.init_cost()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Make it a batch run\n",
    "\n",
    "# Gather all result OD matrices\n",
    "results.extend(Results)\n",
    "OD_matrix_names = []\n",
    "OD_matrix = []\n",
    "\n",
    "for result in results:\n",
    "    OD_matrix_names.append(list(result.keys()))\n",
    "    OD_matrix.append(list(result.values()))\n",
    "\n",
    "OD_matrices_names = [item for sublist in OD_matrix_names for item in sublist]\n",
    "OD_matrices = [dataframe for sublist in OD_matrix for dataframe in sublist]\n",
    "\n",
    "# create dictionary of igraph ID to modified osmID\n",
    "centroid_igraph_to_mod_osmID = {}\n",
    "for i in range(len(centroids)):\n",
    "    centroid_igraph_to_mod_osmID[i] = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['ig'] == centroids[i]]['osmid'].apply(lambda x: 'N'+ (str(x) + '.0').zfill(5)).values[0]\n",
    "\n",
    "\n",
    "# run frank-wolfe on all of them \n",
    "dicts = []\n",
    "for name in OD_matrices_names:\n",
    "    vol2 = None\n",
    "\n",
    "    # Get OD matrix\n",
    "    OD = OD_matrices[OD_matrices_names.index(name)]\n",
    "\n",
    "    # Rename the columns and rows according to the modified osmID \n",
    "    OD = OD.rename(columns = {i : centroid_igraph_to_mod_osmID[i] for i in range(len(OD))}) #rename index of centroid as osmid of centroid\n",
    "    OD.index = OD.columns\n",
    "\n",
    "    # From all centroids to all centroids\n",
    "    origins = OD.columns\n",
    "    destinations = origins\n",
    "    \n",
    "    vol2 = frank_wolfe(nt, OD, origins, destinations)\n",
    "    dicts.append(vol2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute benefit metric for all gaps (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m pd\u001b[39m.\u001b[39mread_csv(file_path, chunksize\u001b[39m=\u001b[39mchunk_size):\n\u001b[1;32m     28\u001b[0m     chunks\u001b[39m.\u001b[39mappend(chunk)\n\u001b[0;32m---> 30\u001b[0m results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49mmap(process_chunk, chunks)\n\u001b[1;32m     31\u001b[0m pool\u001b[39m.\u001b[39mclose()\n\u001b[1;32m     32\u001b[0m pool\u001b[39m.\u001b[39mjoin()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/site-packages/multiprocess/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/site-packages/multiprocess/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/site-packages/multiprocess/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/thesis/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# WITH MULTIPROCESSING \n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    chunk['path'] = chunk.path.apply(eval)\n",
    "    chunk[\"B_star\"] = chunk.apply(lambda x: \n",
    "                                    np.sum([dicts[0][g_df.loc[g_df['id'] == i]['edge'].values[0]] * \\\n",
    "                                            g_igraph.es[i][\"length\"] \\\n",
    "                                            for i in x.path]), \n",
    "                                    axis=1)\n",
    "    chunk[\"B\"] = chunk[\"B_star\"] / chunk[\"length\"]\n",
    "    return chunk\n",
    "\n",
    "# Read the CSV file in chunks using multiprocessing\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Define the file path\n",
    "    file_path = './data/clean/mygaps2.csv'\n",
    "\n",
    "    # Define the chunk size\n",
    "    chunk_size = 500\n",
    "    num_processes = multiprocessing.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    chunks = []\n",
    "    \n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    results = pool.map(process_chunk, chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "# Concatenate the processed chunks into a single DataFrame\n",
    "mygaps = pd.concat(results)\n",
    "\n",
    "# Sort gaps by descending benefit metric\n",
    "mygaps = mygaps.sort_values(by=\"B\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(mygaps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiprocessing part 2\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk):\n",
    "    # Read the CSV file in each process\n",
    "    chunk['path'] = chunk.path.apply(eval)\n",
    "    chunk[\"B_star\"] = chunk.apply(lambda x: \n",
    "                                    np.sum([dicts[0][g_df.loc[g_df['id'] == i]['edge'].values[0]] * \\\n",
    "                                            g_igraph.es[i][\"length\"] \\\n",
    "                                            for i in x.path]), \n",
    "                                    axis=1)\n",
    "    chunk[\"B\"] = chunk[\"B_star\"] / chunk[\"length\"]\n",
    "    return chunk\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define the file path\n",
    "    file_path = './data/clean/mygaps2.csv'\n",
    "\n",
    "    # Define the chunk size\n",
    "    chunk_size = 500\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    results = pool.map(process_chunk, pd.read_csv(file_path, chunksize=chunk_size))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Concatenate the processed chunks into a single DataFrame\n",
    "    mygaps = pd.concat(results)\n",
    "\n",
    "    # Sort gaps by descending benefit metric\n",
    "    mygaps = mygaps.sort_values(by=\"B\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Display the resulting DataFrame\n",
    "    mygaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>length</th>\n",
       "      <th>path_nx</th>\n",
       "      <th>o_nx</th>\n",
       "      <th>d_nx</th>\n",
       "      <th>B_star</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[44691, 40872, 40867, 40863, 40862, 40791, 407...</td>\n",
       "      <td>48.035217</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (6...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>1755.159241</td>\n",
       "      <td>36.539010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[44691, 40872, 40871, 44690, 40706, 40694, 406...</td>\n",
       "      <td>189.035053</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (1...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>5104.326154</td>\n",
       "      <td>27.002009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[44691, 40872, 40871, 44690, 40706, 40694, 406...</td>\n",
       "      <td>190.105948</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (1...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>5104.326154</td>\n",
       "      <td>26.849902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[44691, 40872, 40867, 40863, 40862, 40791, 407...</td>\n",
       "      <td>18.750942</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (6...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[44691, 40872, 40871, 44690, 40860, 40859, 408...</td>\n",
       "      <td>22.766010</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (1...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>[44691, 40872, 40867, 40863, 40862, 40791, 407...</td>\n",
       "      <td>37.974624</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (6...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>[44691, 40872, 40867, 40863, 40862, 40791, 407...</td>\n",
       "      <td>34.159859</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (6...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>[44691, 40872, 40867, 40863, 40862, 40791, 407...</td>\n",
       "      <td>33.783334</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (6...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>[44691, 40872, 40867, 40863, 40862, 40791, 407...</td>\n",
       "      <td>35.468790</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (6...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>[44691, 40872, 40871, 44690, 40706, 40694, 406...</td>\n",
       "      <td>189.593112</td>\n",
       "      <td>[(2660, 605558404), (605558404, 605558413), (1...</td>\n",
       "      <td>[</td>\n",
       "      <td>]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  path      length  \\\n",
       "0    [44691, 40872, 40867, 40863, 40862, 40791, 407...   48.035217   \n",
       "1    [44691, 40872, 40871, 44690, 40706, 40694, 406...  189.035053   \n",
       "2    [44691, 40872, 40871, 44690, 40706, 40694, 406...  190.105948   \n",
       "3    [44691, 40872, 40867, 40863, 40862, 40791, 407...   18.750942   \n",
       "4    [44691, 40872, 40871, 44690, 40860, 40859, 408...   22.766010   \n",
       "..                                                 ...         ...   \n",
       "495  [44691, 40872, 40867, 40863, 40862, 40791, 407...   37.974624   \n",
       "496  [44691, 40872, 40867, 40863, 40862, 40791, 407...   34.159859   \n",
       "497  [44691, 40872, 40867, 40863, 40862, 40791, 407...   33.783334   \n",
       "498  [44691, 40872, 40867, 40863, 40862, 40791, 407...   35.468790   \n",
       "499  [44691, 40872, 40871, 44690, 40706, 40694, 406...  189.593112   \n",
       "\n",
       "                                               path_nx o_nx d_nx       B_star  \\\n",
       "0    [(2660, 605558404), (605558404, 605558413), (6...    [    ]  1755.159241   \n",
       "1    [(2660, 605558404), (605558404, 605558413), (1...    [    ]  5104.326154   \n",
       "2    [(2660, 605558404), (605558404, 605558413), (1...    [    ]  5104.326154   \n",
       "3    [(2660, 605558404), (605558404, 605558413), (6...    [    ]     0.000000   \n",
       "4    [(2660, 605558404), (605558404, 605558413), (1...    [    ]     0.000000   \n",
       "..                                                 ...  ...  ...          ...   \n",
       "495  [(2660, 605558404), (605558404, 605558413), (6...    [    ]     0.000000   \n",
       "496  [(2660, 605558404), (605558404, 605558413), (6...    [    ]     0.000000   \n",
       "497  [(2660, 605558404), (605558404, 605558413), (6...    [    ]     0.000000   \n",
       "498  [(2660, 605558404), (605558404, 605558413), (6...    [    ]     0.000000   \n",
       "499  [(2660, 605558404), (605558404, 605558413), (1...    [    ]     0.000000   \n",
       "\n",
       "             B  \n",
       "0    36.539010  \n",
       "1    27.002009  \n",
       "2    26.849902  \n",
       "3     0.000000  \n",
       "4     0.000000  \n",
       "..         ...  \n",
       "495   0.000000  \n",
       "496   0.000000  \n",
       "497   0.000000  \n",
       "498   0.000000  \n",
       "499   0.000000  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT MULTIPROCESSING\n",
    "mygaps = pd.read_csv('./data/clean/mygaps2.csv', nrows=500)\n",
    "mygaps['path'] = mygaps.path.apply(lambda x: eval(x))\n",
    "\n",
    "mygaps[\"B_star\"] = mygaps.apply(lambda x: \n",
    "                                        np.sum([dicts[0][g_df.loc[g_df['id'] == i]['edge'].values[0]] * \\\n",
    "                                                g_igraph.es[i][\"length\"] \\\n",
    "                                                for i in x.path]), \n",
    "                                        axis = 1)\n",
    "\n",
    "mygaps[\"B\"] = mygaps[\"B_star\"] / mygaps[\"length\"] # B(g) normed to length\n",
    "\n",
    "# sort gaps by descending benefit metric\n",
    "mygaps = mygaps.sort_values(by = \"B\", ascending = False).reset_index(drop = True)\n",
    "mygaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekend / monday plan\n",
    "# read EA stuff, do home stuff and gym over the weekend. NO THESIS!\n",
    "# run the code overnight on Sunday night, see if the multiprocessing thin gets done by Monday morning (doesn't really matter anyway)\n",
    "# do the todos below on Monday\n",
    "\n",
    "\n",
    "\n",
    "# TODO after meeting Trivik\n",
    "# maybe need to NOT normalize number of trips\n",
    "# maybe need to have realistic capacity\n",
    "# normalize the weights if you want but that's it\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
