{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the OD Matrices\n",
    "- Matrix 0: shortest trips between centroids\n",
    "- Baseline: pop density and exp(normalized distance) -> gravity model baseline like Yap et al.\n",
    "- Matrix set 1: equalizing for median income, education level, number of schools and number of jobs SEPARATELY\n",
    "- Matrix set 2: equalizing for different attributes in O and D. O/D equalized for education level/number of schools, median income/number of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import osmnx as nx\n",
    "import shapely\n",
    "import multiprocess as mp\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "import igraph as ig\n",
    "\n",
    "crs_fr = 2154\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (Anastassia)\n",
    "%run -i packages.py\n",
    "def make_attr_dict(*args, **kwargs): \n",
    "    \n",
    "    argCount = len(kwargs)\n",
    "    \n",
    "    if argCount > 0:\n",
    "        attributes = {}\n",
    "        for kwarg in kwargs:\n",
    "            attributes[kwarg] = kwargs.get(kwarg, None)\n",
    "        return attributes\n",
    "    else:\n",
    "        return None # (if no attributes are given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (Jin)\n",
    "# equalize an OD for the same attribute in O and D\n",
    "def equalization_all(od, variable, colname, delta, centroids): \n",
    "    \n",
    "    od_ = od.copy()\n",
    "    variable_ = variable.copy()\n",
    "    \n",
    "    variable_average = np.mean(variable_[colname]) \n",
    "    \n",
    "    variable_['weight'] = variable_[colname].apply(lambda x: (x/variable_average)**-delta)\n",
    "\n",
    "    i =0\n",
    "    for val in variable_['ig']:\n",
    "        weight = variable_.loc[variable_['ig']==val]['weight'].iloc[0]\n",
    "        try:\n",
    "            od_[centroids.index(val)] *= weight \n",
    "            od_.loc[centroids.index(val)] *= weight \n",
    "        except:\n",
    "            continue\n",
    "#             print(val, ' not found')\n",
    "        i +=1\n",
    "    \n",
    "    return od_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function to use the function above in a batch\n",
    "def clean_data_with_od_matrices(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST, delta):\n",
    "    col_tokeep = ['osmid', 'ig', 'CODE_IRIS', COLOFINTEREST]\n",
    "    COLOFINTEREST_df = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['centroid'] == True].copy()\n",
    "    COLOFINTEREST_df = COLOFINTEREST_df[col_tokeep]\n",
    "    \n",
    "    OD_equalization = equalization_all(baseline_df, COLOFINTEREST_df, COLOFINTEREST, delta, centroids)\n",
    "    \n",
    "    OD_equalization_name = \"OD_equalization_\" + COLOFINTEREST + \"_\" + str(delta)\n",
    "    \n",
    "    return {OD_equalization_name: OD_equalization}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (adapted from Jin)\n",
    "# equalize an OD for DIFFFERENT attributes in O and D\n",
    "\n",
    "def equalization_all_2attributes(od, variable, colnameO, colnameD, delta, centroids): \n",
    "    \n",
    "    od_ = od.copy()\n",
    "    variable_ = variable.copy()\n",
    "    \n",
    "    variable_average1 = np.mean(variable_[colnameO])\n",
    "    variable_average2 = np.mean(variable_[colnameD])\n",
    "    \n",
    "    #here we keep -delta because we want to penalize the high values\n",
    "    # e.g low income is prioritized \n",
    "    variable_['weightO'] = variable_[colnameO].apply(lambda x: (x / variable_average1) ** -delta)\n",
    "\n",
    "    # here we use +delta because we want to penalize the low values\n",
    "    # e.g high number of jobs in an area is prioritzed\n",
    "    variable_['weightD'] = variable_[colnameD].apply(lambda x: (x / variable_average2) ** delta) #\n",
    "    \n",
    "    i = 0\n",
    "    for val in variable_['ig']:\n",
    "        weightO = variable_.loc[variable_['ig'] == val]['weightO'].iloc[0]\n",
    "        weightD = variable_.loc[variable_['ig'] == val]['weightD'].iloc[0]\n",
    "        try:\n",
    "            od_.loc[centroids.index(val)] *= weightO #row = origin\n",
    "            od_[centroids.index(val)] *= weightD #column = destination\n",
    "        except:\n",
    "            continue\n",
    "        i += 1\n",
    "    \n",
    "    return od_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function to use the function above in a batch\n",
    "def equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST1, COLOFINTEREST2, delta):\n",
    "    col_tokeep = ['osmid', 'ig', 'CODE_IRIS', COLOFINTEREST1, COLOFINTEREST2]\n",
    "    COLSOFINTEREST_df = nodes_carbike_centroids_RER_complete.loc[nodes_carbike_centroids_RER_complete['centroid'] == True].copy()\n",
    "    COLSOFINTEREST_df = COLSOFINTEREST_df[col_tokeep]\n",
    "    \n",
    "    equalized_od = equalization_all_2attributes(baseline_df, COLSOFINTEREST_df, COLOFINTEREST1, COLOFINTEREST2, delta, centroids)\n",
    "    \n",
    "    equalized_od_name = \"OD_equalization_\" + COLOFINTEREST1 + \"_O_\"+ COLOFINTEREST2 + \"_D_delta_\" + str(delta)\n",
    "    \n",
    "    return {equalized_od_name: equalized_od}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Shapes\n",
    "\n",
    "# GPM outline\n",
    "GPM = gpd.read_file('data/raw/GPM.geojson').to_crs(crs_fr)\n",
    "\n",
    "# IRIS codes and shapes \n",
    "IRIS_GPM = gpd.read_file('data/raw/IRIS_GPM.geojson')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network and adding igraph IDs to the node table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create the network in NetworkX\n",
    "# Retrieve edges\n",
    "edges_with_id = pd.read_csv('data/clean/initial_network_edges.csv')\n",
    "edges_with_id[\"geometry\"] = edges_with_id.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "edges_with_id = gpd.GeoDataFrame(edges_with_id, geometry = 'geometry', crs = 4326).to_crs(2154)\n",
    "\n",
    "# Retrieve nodes\n",
    "nodes_carbike_centroids_RER_complete = pd.read_csv('data/clean/initial_network_nodes_complete.csv')\n",
    "nodes_carbike_centroids_RER_complete[\"geometry\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "nodes_carbike_centroids_RER_complete = gpd.GeoDataFrame(nodes_carbike_centroids_RER_complete, geometry = 'geometry', crs = 2154)\n",
    "\n",
    "# Create the attr_dict\n",
    "nodes_carbike_centroids_RER_complete[\"attr_dict\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: make_attr_dict(\n",
    "                                                                  nodetype = x.nodetype,\n",
    "                                                                  centroid = x.centroid,\n",
    "                                                                  RER = x.RER,\n",
    "                                                                  IRIS = x.CODE_IRIS,\n",
    "                                                                  pop_dens = x.pop_density,\n",
    "                                                                  active_pop_density = x.active_pop_density,\n",
    "                                                                  school_pop_density = x.school_pop_density,\n",
    "                                                                  school_count = x.school_count,\n",
    "                                                                  num_jobs = x.num_jobs,\n",
    "                                                                  ),\n",
    "                                                                  axis = 1) \n",
    "\n",
    "# Create Graph with all nodes and edges\n",
    "G = nx.from_pandas_edgelist(edges_with_id, source='x', target='y', edge_attr=True)\n",
    "G.add_nodes_from(nodes_carbike_centroids_RER_complete.loc[:,[\"osmid\", \"attr_dict\"]].itertuples(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Moving from NetworkX to igraph\n",
    "g_igraph = ig.Graph()\n",
    "networkx_graph = G\n",
    "g_igraph = ig.Graph.from_networkx(networkx_graph)\n",
    "\n",
    "# eids: \"conversion table\" for edge ids from igraph to nx \n",
    "eids_nx = [tuple(sorted(literal_eval(g_igraph.es(i)[\"edge_id\"][0]))) for i in range(len(g_igraph.es))]\n",
    "eids_ig = [i for i in range(len(g_igraph.es))]\n",
    "eids_conv = pd.DataFrame({\"nx\": eids_nx, \"ig\": eids_ig})\n",
    "\n",
    "# nids: \"conversion table\" for node ids from igraph to nx\n",
    "nids_nx = [g_igraph.vs(i)[\"_nx_name\"][0] for i in range(len(g_igraph.vs))]\n",
    "nids_ig = [i for i in range(len(g_igraph.vs))]\n",
    "nids_conv = pd.DataFrame({\"nx\": nids_nx, \"ig\": nids_ig})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nids_conv['nx'] = nids_conv['nx'].astype(int)\n",
    "\n",
    "# combine the conversion table with nodes_carbike_centroids_RER_complete\n",
    "nodes_carbike_centroids_RER_complete = nodes_carbike_centroids_RER_complete.merge(nids_conv, left_on = \"osmid\", right_on = \"nx\", how = \"left\")\n",
    "nodes_carbike_centroids_RER_complete = nodes_carbike_centroids_RER_complete.drop(columns = [\"nx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate centroids\n",
    "from itertools import combinations\n",
    "seq = g_igraph.vs.select(centroid_eq = True)\n",
    "centroids = [v.index for v in seq]\n",
    "centroids = centroids[0:200] #for testing purposes \n",
    "node_combinations = list(combinations(centroids, 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix 0: shortest path between each pair of centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "<timed exec>:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "<timed exec>:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "<timed exec>:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "<timed exec>:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "<timed exec>:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "<timed exec>:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n",
      "<timed exec>:5: DeprecationWarning: Graph.shortest_paths() is deprecated; use Graph.distances() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200)\n",
      "CPU times: user 337 ms, sys: 76.1 ms, total: 413 ms\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create OD matrix\n",
    "def process_node(args):\n",
    "    start_node, end_node = args\n",
    "    global g_igraph\n",
    "    shortest_path_length = g_igraph.shortest_paths_dijkstra(source=start_node, target=end_node, weights='weight')[0][0]\n",
    "    return (start_node, end_node, shortest_path_length)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Number of processes (cores) to use for parallel processing\n",
    "    num_processes = mp.cpu_count()\n",
    "    global g_igraph\n",
    "\n",
    "    # Create a pool of processes\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Apply the function to each node combination using parallel processing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Create a dictionary to store the shortest path lengths\n",
    "    output = {}\n",
    "    for start_node, end_node, shortest_path_length in results:\n",
    "        if start_node not in output:\n",
    "            output[start_node] = {}\n",
    "        output[start_node][end_node] = shortest_path_length\n",
    "\n",
    "    # Create an empty adjacency matrix\n",
    "    matrix = np.zeros((len(centroids), len(centroids)))\n",
    "\n",
    "    # Fill the adjacency matrix with shortest path lengths\n",
    "    for i, start_node in enumerate(centroids):\n",
    "        for j, end_node in enumerate(centroids):\n",
    "            if start_node in output and end_node in output[start_node]:\n",
    "                matrix[i, j] = output[start_node][end_node]\n",
    "                matrix[j, i] = output[start_node][end_node]\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(matrix.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: population densities and exponential term with normalised distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 544 ms, sys: 60.2 ms, total: 604 ms\n",
      "Wall time: 822 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_node(args):\n",
    "    global matrix\n",
    "    o, d = args\n",
    "    if o == d:\n",
    "        return (o, d, 0)\n",
    "    else:\n",
    "        normalized_dist = matrix[o][d] / matrix.max()\n",
    "        demand = (\n",
    "            (g_igraph.vs[centroids[o]]['pop_dens'] * g_igraph.vs[centroids[d]]['pop_dens'])\n",
    "            * dist_decay * np.exp(-1 * normalized_dist)\n",
    "        )\n",
    "        return (o, d, demand)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    baseline = np.zeros((len(centroids), len(centroids)))\n",
    "    maxtrips = 100\n",
    "    dist_decay = 1\n",
    "\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Create node combinations\n",
    "    node_combinations = [(o, d) for o in range(len(centroids)) for d in range(len(centroids))]\n",
    "\n",
    "    # Calculate demand for each node combination using multiprocessing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Update baseline matrix with calculated demand\n",
    "    for o, d, demand in results:\n",
    "        baseline[o][d] = demand\n",
    "\n",
    "    # Normalize the matrix to the number of maxtrips\n",
    "    baseline = ((baseline / baseline.max()) * maxtrips)\n",
    "\n",
    "    # Round up to ensure each journey is made at least once\n",
    "    baseline = np.ceil(baseline).astype(int)\n",
    "    baseline_df = pd.DataFrame(baseline)\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 758 ms, sys: 27.2 ms, total: 785 ms\n",
      "Wall time: 788 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate demand between each origin and destination\n",
    "# NO MULTIPROCESSING\n",
    "baseline = np.zeros((len(centroids), len(centroids)))\n",
    "maxtrips = 100\n",
    "dist_decay = 1\n",
    "\n",
    "for o in range(0, len(centroids)):\n",
    "    for d in range(0, len(centroids)):\n",
    "        if o == d:\n",
    "            # do not insert demand down the spine - no trips where origin = destination\n",
    "            baseline[o][d] = 0\n",
    "        else:\n",
    "            # normalize the current travel time versus the largest travel time between nodes in the matrix\n",
    "            normalized_dist = matrix[o][d] / matrix.max()\n",
    "\n",
    "            #  here, demand is a function of the product of the population of the origin and\n",
    "            #  the destination - but reduced by the distance between them. 'Gravity demand'\n",
    "            baseline[o][d] = ((g_igraph.vs[centroids[o]]['pop_dens'] * g_igraph.vs[centroids[d]]['pop_dens']) * dist_decay * np.exp(-1 * normalized_dist))\n",
    "\n",
    "# we normalize the matrix to the number of maxtrips\n",
    "baseline = ((baseline / baseline.max()) * maxtrips)\n",
    "\n",
    "# we round up - to ensure each journey is made at least once\n",
    "baseline = np.ceil(baseline).astype(int)\n",
    "baseline_df = pd.DataFrame(baseline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 1: equalizing for median income, education level, number of schools and number of jobs SEPARATELY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.4 ms, sys: 45.5 ms, total: 74.9 ms\n",
      "Wall time: 2.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_data(args):\n",
    "    COLOFINTEREST, delta = args\n",
    "    result = clean_data_with_od_matrices(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST, delta)\n",
    "    return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    num_processes = mp.cpu_count()\n",
    "\n",
    "    # Create a pool of processes\n",
    "    pool = mp.Pool(processes=num_processes)    \n",
    "    COLOFINTEREST_list = ['median_income', 'school_count', 'num_jobs']\n",
    "    delta_list = [0.5, 1, 1.5]\n",
    "    arguments = [(COLOFINTEREST, delta) for COLOFINTEREST in COLOFINTEREST_list for delta in delta_list]\n",
    "    results = pool.map(process_data, arguments)\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Set 2: equalize for O/D attributes median income/ number of jobs, education level/number of schools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.4 ms, sys: 40.4 ms, total: 60.8 ms\n",
      "Wall time: 4.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def process_combination(combination):\n",
    "    COLOFINTEREST1, COLOFINTEREST2 = combination\n",
    "    delta_list = [0.5, 1, 1.5]\n",
    "    results = []\n",
    "    for delta in delta_list:\n",
    "        result = equalization_with_2attributes(nodes_carbike_centroids_RER_complete, baseline_df, centroids, COLOFINTEREST1, COLOFINTEREST2, delta)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    combinations = [['median_income', 'num_jobs']]  # [['median_income', 'num_jobs'], ['edu_level', 'num_schools']]\n",
    "    num_processes = mp.cpu_count()\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "    results = pool.map(process_combination, combinations)\n",
    "    pool.close()\n",
    "    pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.640403005099992"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)\n",
    "(stop - start)/60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
