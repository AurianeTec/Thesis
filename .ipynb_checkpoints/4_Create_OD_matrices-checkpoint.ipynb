{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the OD Matrices\n",
    "- Baseline: OD matrix with limitation factor lambda (Anastassia)\n",
    "- Scenario 1: OD matrix  with pop density, deterrence factor and limitation factor lambda (Baseline Yap et al. + Anastassia)\n",
    "- Scenario 2/3/4: OD matrix with vulnerability  (median income, education level, # jobs within 30min with competition), pop density and deterrence factor lambda (modified Yap et al. + Anastassia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import osmnx as nx\n",
    "import shapely\n",
    "import multiprocess as mp\n",
    "import numpy as np\n",
    "import math\n",
    "import igraph as ig\n",
    "\n",
    "crs_fr = 2154\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Custom function (Anastassia)\n",
    "%run -i packages.py\n",
    "def make_attr_dict(*args, **kwargs): \n",
    "    \n",
    "    argCount = len(kwargs)\n",
    "    \n",
    "    if argCount > 0:\n",
    "        attributes = {}\n",
    "        for kwarg in kwargs:\n",
    "            attributes[kwarg] = kwargs.get(kwarg, None)\n",
    "        return attributes\n",
    "    else:\n",
    "        return None # (if no attributes are given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Shapes\n",
    "\n",
    "# GPM outline\n",
    "GPM = gpd.read_file('data/raw/GPM.geojson').to_crs(crs_fr)\n",
    "\n",
    "# IRIS codes and shapes \n",
    "IRIS_GPM = gpd.read_file('data/raw/IRIS_GPM.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the network and adding igraph IDs to the node table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-20\n"
     ]
    }
   ],
   "source": [
    "#--- Create the network\n",
    "# Retrieve edges\n",
    "edges_with_id = pd.read_csv('data/clean/initial_network_edges_complete.csv')\n",
    "edges_with_id[\"geometry\"] = edges_with_id.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "edges_with_id = gpd.GeoDataFrame(edges_with_id, geometry = 'geometry', crs = 4326).to_crs(2154)\n",
    "\n",
    "# Retrieve nodes\n",
    "nodes_carbike_centroids_RER_complete = pd.read_csv('data/clean/initial_network_nodes_complete.csv')\n",
    "nodes_carbike_centroids_RER_complete[\"geometry\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: shapely.wkt.loads(x.geometry), axis = 1)\n",
    "nodes_carbike_centroids_RER_complete = gpd.GeoDataFrame(nodes_carbike_centroids_RER_complete, geometry = 'geometry', crs = 2154)\n",
    "\n",
    "# Create the attr_dict\n",
    "nodes_carbike_centroids_RER_complete[\"attr_dict\"] = nodes_carbike_centroids_RER_complete.apply(lambda x: make_attr_dict(\n",
    "                                                                  nodetype = x.nodetype,\n",
    "                                                                  centroid = x.centroid,\n",
    "                                                                  RER = x.RER,\n",
    "                                                                  IRIS = x.CODE_IRIS,\n",
    "                                                                #   pop_dens = x.pop_density\n",
    "                                                                  ),\n",
    "                                                                  axis = 1) \n",
    "\n",
    "#--- Create Graph with all nodes and edges\n",
    "G = nx.from_pandas_edgelist(edges_with_id, source='x', target='y', edge_attr=True)\n",
    "G.add_nodes_from(nodes_carbike_centroids_RER_complete.loc[:,[\"osmid\", \"attr_dict\"]].itertuples(index = False))\n",
    "\n",
    "#--- Check if all nodes and edges are present\n",
    "# Both should be 0\n",
    "print(len(G.nodes()) - len(nodes_carbike_centroids_RER_complete))\n",
    "print(len(G.edges()) - len(edges_with_id))\n",
    "\n",
    "#--- Moving from NetworkX to igraph\n",
    "g_igraph = ig.Graph()\n",
    "networkx_graph = G\n",
    "g_igraph = ig.Graph.from_networkx(networkx_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic OD Matrix: shortest path between each pair of centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate centroids\n",
    "\n",
    "seq = g_igraph.vs.select(centroid_eq = True)\n",
    "centroids = [v.index for v in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39m# Generate combinations of nodes for processing\u001b[39;00m\n\u001b[1;32m     23\u001b[0m node_combinations \u001b[39m=\u001b[39m [(start_node, end_node) \u001b[39mfor\u001b[39;00m start_node \u001b[39min\u001b[39;00m centroids \u001b[39mfor\u001b[39;00m end_node \u001b[39min\u001b[39;00m centroids \u001b[39mif\u001b[39;00m start_node \u001b[39m!=\u001b[39m end_node]\n\u001b[0;32m---> 24\u001b[0m node_combinations \u001b[39m=\u001b[39m remove_duplicates(node_combinations)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Apply the function to each node combination using parallel processing\u001b[39;00m\n\u001b[1;32m     27\u001b[0m results \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mmap(process_node, node_combinations)\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36mremove_duplicates\u001b[0;34m(lst)\u001b[0m\n\u001b[1;32m      4\u001b[0m unique_lst \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m lst:\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39msorted\u001b[39m(sublist) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39msorted\u001b[39m(unique_sublist) \u001b[39mfor\u001b[39;00m unique_sublist \u001b[39min\u001b[39;00m unique_lst]:\n\u001b[1;32m      7\u001b[0m         unique_lst\u001b[39m.\u001b[39mappend(sublist)\n\u001b[1;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m unique_lst\n",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m unique_lst \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m lst:\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39msorted\u001b[39m(sublist) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39msorted\u001b[39;49m(unique_sublist) \u001b[39mfor\u001b[39;00m unique_sublist \u001b[39min\u001b[39;00m unique_lst]:\n\u001b[1;32m      7\u001b[0m         unique_lst\u001b[39m.\u001b[39mappend(sublist)\n\u001b[1;32m      8\u001b[0m \u001b[39mreturn\u001b[39;00m unique_lst\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create OD matrix\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    unique_lst = []\n",
    "    for sublist in lst:\n",
    "        if sorted(sublist) not in [sorted(unique_sublist) for unique_sublist in unique_lst]:\n",
    "            unique_lst.append(sublist)\n",
    "    return unique_lst\n",
    "\n",
    "def process_node(args):\n",
    "    start_node, end_node = args\n",
    "    shortest_path_length = g_igraph.shortest_paths_dijkstra(source=start_node, target=end_node, weights='weight')[0][0]\n",
    "    return (start_node, end_node, shortest_path_length)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Number of processes (cores) to use for parallel processing\n",
    "    num_processes = 4\n",
    "\n",
    "    # Create a pool of processes\n",
    "    pool = mp.Pool(processes=num_processes)\n",
    "\n",
    "    # Generate combinations of nodes for processing\n",
    "    node_combinations = [(start_node, end_node) for start_node in centroids for end_node in centroids if start_node != end_node]\n",
    "    node_combinations = remove_duplicates(node_combinations)\n",
    "\n",
    "    # Apply the function to each node combination using parallel processing\n",
    "    results = pool.map(process_node, node_combinations)\n",
    "\n",
    "    # Create a dictionary to store the shortest path lengths\n",
    "    output = {}\n",
    "    for start_node, end_node, shortest_path_length in results:\n",
    "        if start_node not in output:\n",
    "            output[start_node] = {}\n",
    "        output[start_node][end_node] = shortest_path_length\n",
    "\n",
    "    # Create an empty adjacency matrix\n",
    "    matrix = np.zeros((len(centroids), len(centroids)))\n",
    "\n",
    "    # Fill the adjacency matrix with shortest path lengths\n",
    "    for i, start_node in enumerate(centroids):\n",
    "        for j, end_node in enumerate(centroids):\n",
    "            if start_node in output and end_node in output[start_node]:\n",
    "                matrix[i, j] = output[start_node][end_node]\n",
    "                matrix[j, i] = output[start_node][end_node]\n",
    "\n",
    "    # Close the pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base OD matrix: population densities and exponential term with normalised distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Add population density as a node attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Create OD matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
